```{r}
library(lubridate)
library(rmarkdown)
library(knitr)
library(ggplot2)
library(GGally)
```


```{r}
#loading dataset. Dataset need to be in the same folder of the project.
load_dataset = function(data_folder, file){
  PATH = paste(getwd(), data_folder, file, sep="/")
}
```

```{r}
tripdata_2019_r = read.csv(load_dataset("SL_dataset", "tripdata_2019_r.csv"))
```


```{r}
par(mfrow=c(1,2))

freq_month_19 = barplot(months_19, 
     main = "Freq during months",
     xlab = "Months",
     ylab = "Freq",
     col = "light Blue")
text(x = freq_month_19, y = months_19 + 0.5, labels = months_19, cex=0.8)


mean_dur_19 = c()

for (val in 1:12){
  mean_dur_19[val] =  round(mean(tripdata_2019_r$tripduration[tripdata_2019_r$month == val])/60,2)
}

min_per_month_19 = barplot(mean_dur_19, 
     main = "Avg (min) per month",
     xlab = "Months",
     ylab = "Mins",
     col = "light green",
     names.arg=c("1","2","3","4","5", "6", "7", "8", "9", "10", "11", "12"))
text(x = min_per_month_19, y = mean_dur_19 + 0.5, labels = mean_dur_19, cex=0.7)
```

```{r}
cat("min age:", min(tripdata_2019_r$age))
cat("\nmax age:", max(tripdata_2019_r$age))
cat("\nusers >80y:", nrow(tripdata_2019_r[tripdata_2019_r$age > 80,]))
```

```{r}
hist(tripdata_2019_r$age, 
     breaks = 20,
     labels = TRUE)


dens = density(tripdata_2019_r$age)
plot(dens)

boxplot(tripdata_2019_r$age)
```


```{r}
sub_data = tripdata_2019_r[tripdata_2019_r$age <= 80,]
```

```{r}
X = subset(sub_data, select = c(tripduration, starttime, startday, month, start.station.id, end.station.id))

#our Y
age_groups = cut(sub_data$age, breaks=c(16, 25, 35, 45, 55, 65, 75, 80), right = FALSE, labels = FALSE)
```


```{r}
nrow(X)
length(age_groups)
```


```{r}
model = lm(age_groups~., data=X)
summary(model)
```
it can be seen that p-value of the F-statistic is < 2.2e-16, which is highly significant. This means that, at least, one of the predictor variables is significantly related to the outcome variable.

```{r}
summary(model)$coefficient
```
the t-statistic evaluates whether or not there is significant association between the predictor and the outcome variable, that is whether the beta coefficient of the predictor is significantly different from zero.

It seems like that the "startday" and "start.station.id" are not significant, thus we can remove them.

```{r}
model = lm(age_groups~tripduration + starttime + month + end.station.id, data=X)
summary(model)
```
R2 represents the proportion of variance, in the outcome variable y, that may be predicted by knowing the value of the x variables. An R2 value close to 1 indicates that the model explains a large portion of the variance in the outcome variable.

A problem with the R2, is that, it will always increase when more variables are added to the model, even if those variables are only weakly associated with the response. A solution is to adjust the R2 by taking into account the number of predictor variables.

The adjustment in the “Adjusted R Square” value in the summary output is a correction for the number of x variables included in the prediction model.

In our case, the adjusted R2 = 0.008, meaning that “0.08% of the variance in the measure can be predicted by starttime and month"
```{r}
confint(model)
```

The RSE estimate gives a measure of error of prediction. The lower the RSE, the more accurate the model.
The error rate can be estimated by dividing the RSE by the mean outcome variable:
```{r}
#not correct
sigma(model)/mean(sub_data$age)
```


Test with all variables and not with subset
```{r}
sub_data = tripdata_2019_r[tripdata_2019_r$age <= 80,]
#our Y
age_groups = cut(sub_data$age, breaks=c(16, 25, 35, 45, 55, 65, 75, 80), right = FALSE, labels = FALSE)

sub_data = subset(sub_data, select = -c(X))

nrow(sub_data)
length(age_groups)
```
```{r}
#questo sembra essere il subset migliore
model = lm(age_groups~. -year -birth.year -age -startday -end.station.id -stopday -start.station.id -bikeid -tripduration, data=sub_data)
summary(model)
```

```{r}
confint(model)
```
I mean, siamo passati dallo spiegare lo 0.08% della varianza al 2.4%, ma non so se sia la strada giusta


```{r}
library(broom)

mod = augment(model)

qqnorm(mod$.resid)
qqline(mod$.resid)
```

```{r}
plot(fitted(model), residuals(model), col="gray40", xlab="fitted values", ylab="residuals")
lines(loess.smooth(fitted(model), residuals(model)), col="blue", lwd=2)
```

```{r}

ggpairs(X)
```


```{r}
adv = read.csv(load_dataset("SL_dataset", "Advertising.csv"))
```



```{r}
#sub_data = subset(sub_data, select = -c(year, X, birth.year))
sub_data = subset(sub_data, select = -c(X.1))
round(var(sub_data),2)
```


```{r}
# ++++++++++++++++++++++++++++
# flattenCorrMatrix
# ++++++++++++++++++++++++++++
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}
```


```{r}
library(Hmisc)
res2<-rcorr(as.matrix(sub_data))
a = as.data.frame(flattenCorrMatrix(res2$r, res2$P))
```

```{r}
res = cor(sub_data)
```

```{r}
library(corrplot)
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

```{r}
par(mfrow=c(1,2))
plot(sub_data$start.station.longitude, sub_data$end.station.longitude)
plot(sub_data$start.station.latitude, sub_data$end.station.latitude)

```





```{r}
plot(round(sub_data$starttime/3600,2), round(sub_data$stoptime/3600,2))
```

































