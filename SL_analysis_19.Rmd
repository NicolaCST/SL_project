---
title: "SL_proj_doc"
author: "Nicola Cassetta, Andrea di Trani"
date: '2022-09-11'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Project Introduction**

*BlueBikes* is a public bike sharing system born in July 2011 in Boston. It has grown exponentially over the years:

-   From 3,203 annual members in 2011 to 21,261 in 2019
-   From 610 bicycles in 2011 to 3,500+ in 2019

The system is simple: a user can pick up a bike at any station dock, ride it for a specific amount of time, and then return it to any station for re-docking. Users can also choose to subscribe to the service via an annual (or monthly) plan. However, despite the constant growth of the system, the year 2020 was unlike any before, and like most things, bike share changed radically.

**Project structure**

With this project we want to analyze the impact of the COVID pandemic on the habits of BlueBike's users. Moreover, considering the fact that the dataset of the year 2020 contains many missing values (Gender: 79% missing, BirthYear: 79% missing), we will also try to predict these missing values via some machine learning models.

The aim of the project is twofold, thus it is organized as follow:

1.  Firts we will discuss the main differences between the datasets and the preprocessing steps.

2.  Then we will analyze the 2019 dataset first, looking for trends and habits of pre-pandemic users.

3.  As last step for the 2019, we will use some statistical tool, together with some machine learning techniques, to predict the age group of a user, and its gender.

4.  We will repeat the same EDA process for the 2020, and then we're going to compare the informations among the two years after predicting the missing values with the models build in the previous steps.

**Dataset analysis**

For this project we used two datasets, both available for free on Kaggle:

-   2019 BlueBikes tripdata: 2.52 million of trips, described with 16 attributes

-   2020 BlueBikes tripdata: 2 million of trips, described with 17 attributes

| Attribute name          | Type      | Description                                           |
|:------------------|-------------------|----------------------------------|
| Tripduration            | Integer   | Duration of the trip in seconds                       |
| Starttime               | Integer   | Start time and date of trip                           |
| Stoptime                | Integer   | Stop time and date of trip                            |
| Start_station_id        | Integer   | Unique ID of station - start                          |
| Start_station_name      | Character | Name of the station the trip started at               |
| Start_station_latitude  | Numeric   | Latitude of start station of trip                     |
| Start_station_longitude | Numeric   | Longitude of start station of trip                    |
| End_station_id          | Integer   | Unique ID of station - end                            |
| End_station_name        | Character | Name of the station the trip ended at                 |
| End_station_latitude    | Numeric   | Latitude of end station of trip                       |
| End_station_longitude   | Numeric   | Longitude of end station of trip                      |
| BikeID                  | Integer   | Unique ID of bike used for trip                       |
| UserType                | Character | Type of user: "Customer" or "Subscriber"              |
| Year                    | Integer   | Year of when the trip took place                      |
| Month                   | Integer   | Month of when the trip took place                     |
| Gender                  | Integer   | Gender of the user. 0 = Unknown, 1 = Male, 2 = Female |

As mentioned above, the year 2020 had another attribute: *"PostalCode".* In order to be consistent with the analysis, we removed this field to match the same structure of the past year. Moreover, the trips of December 2020 were missing, thus, we downloaded the report of December2020 from the BlueBikes website, and added the missing values.

**Preprocessing**

In order to perform a solid analysis, we've arranged an initial data preprocessing phase. Here we present some of the choices derived from this exploration:

-   Rather than seconds, we decided to use minutes

-   We've rearranged the Starttime (and Stoptime) format "*2019-12-01 00:01:25.3240*" by splitting it in "StartDate" and "StartTime". Then we splitted "StartDate" into "StartDay" / "StartMonth" / "StartYear" and discarded month and year (repeated data)

-   Instead of Char type, the UserType info is expressed via a number (0: Customer, 1:Subscriber)

-   We removed all the trips with duration \<1m

-   We removed all the trips that started in a month and finished in the next one (ex. start as 31/07 23:59 and end 01/08 00:30). This specific kind of trip amount to 0.035% of the total.

-   We removed a suspicious station (latitude and longitude = 0Â°, this location is named as "Null Island", and is located in international waters in the Atlantic Ocean, roughly 600 km off the coast of West Africa, in the Gulf of Guinea)

-   We added a new field, named "Age", and filled it with the difference between the year of the trip and the birth year of the user.

-   The new field "Age" brought to light some suspicious values (max age: 133). To get rid of these values, we divided the dataset into two sub-dataset: the first, considered the main one, with a max age \<= 85, the second one, with age \> 85 that will be discarded (this type of entries amount to the 0.0003% of the total data).

-   In order to perform a more robust classification on the age, we rearranged the users age into "*Age Groups*":

    -   [16, 30) - Group 1

    -   [30, 45) - Group 2

    -   [45, 65) - Group 3

    -   [65, 85] - Group 4

-   With these new information we were able to discard some other fields: Age, BirthYear, Year, Start_station_name and End_station_name

**Import and Utilities**

```{r}
#needed libraries
library(lubridate)
library(rmarkdown)
library(knitr)
library(ggplot2)
library(GGally)
library(class)
library(Hmisc)
library(corrplot)
library(correlation)
library(gridExtra)
library(class)
library(randomForest)
library(mda)
library(klaR)
library(dplyr)
library(pROC)
library(nnet)
library(caret)
library(tidyverse)
library(MASS)
library(e1071)
library(kableExtra)

set.seed(42)
```

```{r}
# UTILITY FUNCTIONS

#loading dataset. Dataset need to be in the same folder of the project.
load_dataset = function(data_folder, file){
  PATH = paste(getwd(), data_folder, file, sep="/")
}


##the normalization function is created
nor = function(x) { (x -min(x))/(max(x)-min(x))   }


##this function divides the correct predictions by total number of predictions that tell us how accurate teh model is.
accuracy = function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
precision = function(x){(diag(x)/((rowSums(x)))) * 100}
recall = function(x){(diag(x)/((colSums(x)))) * 100}


sample_sub <- function(data, perc){
  
  num = nrow(data) * as.numeric(perc)
  ran = sample(1:num, 0.95 * num)

  a_s = data[ran,]
  diff = setdiff(data,a_s)
  
  ran_test = sample(1:nrow(diff), 0.05 * num)
  a_t = diff[ran_test,]

  return(list(a_s, a_t))
}


#New version of the dataset gen function
#data: dataset to be splitted
#perc_m/f: list of 4 values
get_balanced_v2 <- function(data, perc_m, perc_f){
  
    #Balance GENDER & AGE GROUPS
    f = subset(data, gender == 2)
    m = subset(data, gender == 1)
    
    gend = list(f,m)
    
    full_train = vector(mode='list', length=2)
    full_test = vector(mode='list', length=2)
    
    for (i in (1:length(gend))){
      
      if (i == 1){
        perc = perc_f
      }else{
        perc = perc_m
      }
      
      a = subset(as.data.frame(gend[[i]]), (age_groups == 1))
      full_a = sample_sub(a, perc[1])
      
      b = subset(as.data.frame(gend[[i]]), (age_groups == 2))
      full_b = sample_sub(b, perc[2])
      
      c = subset(as.data.frame(gend[[i]]), (age_groups == 3))
      full_c = sample_sub(c, perc[3])
      
      d = subset(as.data.frame(gend[[i]]), (age_groups == 4))
      full_d = sample_sub(d, perc[4])
      
      data_train = rbind(full_a[[1]], full_b[[1]], full_c[[1]], full_d[[1]])
      data_test = rbind(full_a[[2]], full_b[[2]], full_c[[2]], full_d[[2]])
    
      full_train[[i]] = data_train
      full_test[[i]] = data_test
      
    }   
    
    data_train = as.data.frame(rbind(full_train[[1]], full_train[[2]]))
    data_test = as.data.frame(rbind(full_test[[1]], full_test[[2]]))
  
    rm(full_a, full_b, full_c, full_d, full_train, full_test, a, b, c, d, f, m, gend)
    
    return(list(data_train, data_test)) 
  
}



add_smote_sample <- function(data, age, gen, k_neig=20, times){
  
  t = data[data$age_groups == age & data$gender == gen,]
  
  smote_result22 = smotefamily::SMOTE(t[,1:12], target = t$age_groups, K = k_neig, dup_size = times)
  y = smote_result22$data
  
  colnames(y)[colnames(y) == 'class'] <- 'age_groups'
  y$age_groups = as.numeric(y$age_groups)
  
  y = setdiff(y, t)
  
  data = rbind(data, y)
  
  return(data)
  
}



table_gender_age <- function(train, test){
  cat("TAB TRAIN")
  print(table(train$gender, train$age_groups))
  
  cat("\n", "TAB TEST")
  print(table(test$gender, test$age_groups))
}



# ++++++++++++++++++++++++++++
# flattenCorrMatrix
# ++++++++++++++++++++++++++++
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}


rstudio_viewer <- function(file_name, file_path = NULL) {
    temporary_file <- tempfile()
    dir.create(temporary_file)
    html_file <- file.path(temporary_file, file_name)
    current_path <- ifelse(is.null(file_path),
                           getwd(),
                           path.expand(file_path))
    file.copy(file.path(current_path, file_name), html_file)
    rstudioapi::viewer(html_file)
}

plot_stats <- function(tab, mode="test", cf=F){
  
  cat("\n", "---------", mode, "---------", "\n")
  
  cat("\nAccuracy: ", round(accuracy(tab),2))
  cat("\nPrecision: ", round(precision(tab),2))
  cat("\nRecall: ", round(recall(tab),2), "\n")
  
  if(cf == T){
      cat("\n", "---------", "CF", "---------", "\n")
      tab
  }

}


our_palette = c(rgb(1, 0, 0), 
              rgb(0, 1, 0), 
              rgb(0, 0, 1), 
              rgb(1, 0.5, 0))

months_list = c("1","2","3","4","5", "6", "7", "8", "9", "10", "11", "12")


get_perc <- function(mode = "full"){
  
  full = switch(mode,
                "full" = list( c(1, 1, 1, 1), c(1, 1, 1, 1) ), 
                "half" = list( c(0.5, 0.5, 0.5, 0.5), c(1, 1, 1, 1) ),
                "100k" = list( c(0.12, 0.2, 0.5, 1), c(0.3, 0.7, 1, 1) ),
                "50k" = list( c(0.06, 0.1, 0.25, 1), c(0.15, 0.35, 0.7, 1) ),
                "10k" = list( c(0.03, 0.05, 0.1, 0.8), c(0.05, 0.15, 0.3, 1) ))
  
  return(full)
}


cool_cf <- function(pred, true){
  
  cm <- confusionMatrix(factor(pred), factor(true), dnn = c("Prediction", "GroundT"))

  plt <- as.data.frame(cm$table)
  plt$Prediction <- factor(plt$Prediction, levels=rev(levels(plt$Prediction)))
  
  
  ggplot(plt, aes(GroundT, Prediction, fill= Freq)) +
          geom_tile() + geom_text(aes(label=Freq)) +
          labs(x = "Ground T",y = "Prediction") +
          scale_fill_gradient(low="white", high="#009194") +
          scale_x_discrete(labels=c("Class_1","Class_2","Class_3","Class_4")) +
          scale_y_discrete(labels=c("Class_4","Class_3","Class_2","Class_1"))
  
}


```

**Data Load**

Here we load and shuffle the dataset for the 2019

```{r}
tripdata_2019_r = read.csv(load_dataset("SL_dataset", "tripdata_2019_r.csv"))
tripdata_2019_r = tripdata_2019_r[sample(1:nrow(tripdata_2019_r)),]
```

**EDA on 2019**

As mentioned in the Preprocessing section, it is possible to see some outliers in the age field (probably mainly caused by the users during the registration to the system)

```{r}

par(mfrow=c(1,2))

hist(tripdata_2019_r$age,
     breaks = 20,
     xlab = "Age",
     main = "Age hist 2019",
     col = "lightblue",
     freq = FALSE)

lines(density(tripdata_2019_r$age, adjust = 3), # density plot
 lwd = 2, # thickness of line
 col = "chocolate3")


boxplot(tripdata_2019_r$age,
        ylab = "Age",
        main = "Boxplot age 2019",
        col = "lightblue",
        outcol = "red")

```

From these plots it is possible to see that users with age greater than 75 are considered as "*outliers*". To be sure of this statement we also checked the average lifespan of the Boston's citizens in the 2019 - 2020 from their government website, which results to be around 80 years.

```{r}
cat("min age:", min(tripdata_2019_r$age))
cat("\nmax age:", max(tripdata_2019_r$age))
cat("\nusers >85y:", nrow(tripdata_2019_r[tripdata_2019_r$age > 85,]))
cat("\nusers >100y:", nrow(tripdata_2019_r[tripdata_2019_r$age > 100,]))
cat("\nusers <=85:", nrow(tripdata_2019_r[tripdata_2019_r$age <= 85 & tripdata_2019_r$age > 0,]))
```

### Age groups analysis

In this section we're going to split the ages of the users in 4 groups, as mentioned in the preprocessing section. Moreover, we also provide a normalization of the fields: tripduration, starttime, stoptime along with the exclusion of some fields - like: age, birth.year and year - since at this point they do not provide any useful (or new) information

```{r}
#Split in age groups, remove useless fields, and use Minutes

split_ages = c(16, 30, 45, 65, 85)

data_u85 = tripdata_2019_r[tripdata_2019_r$age <= 85,]
data_u85$age_groups = cut(data_u85$age, breaks=split_ages, include.lowest = TRUE, labels = FALSE)

data_o85 = tripdata_2019_r[tripdata_2019_r$age > 85,]
data_o85$age_groups = 0

data_u85 = subset(data_u85, select = -c(age, birth.year, year))
data_o85 = subset(data_o85, select = -c(age, birth.year, year))

data_u85$tripduration = round(data_u85$tripduration/60,2)
data_u85$starttime = round(data_u85$starttime/60,2)
data_u85$stoptime = round(data_u85$stoptime/60,2)

data_u85[c("starttime", "stoptime")] = lapply(data_u85[c("starttime", "stoptime")], nor)
```

```{r}
barplot_age_group = barplot(table(data_u85$age_groups), 
     main = "Age groups count",
     xlab = "Age group",
     ylab = "Freq",
     col = viridis(4))

legend("topright", 
       legend = c("Group 1: 16-30", 
                  "Group 2: 31-45", 
                  "Group 3: 46-65", 
                  "Group 4: 65-85"),
       fill = viridis(4))
```

```{r}
tab_group = as.data.frame(table((data_u85$age_groups)))
tab_group$perc = round((tab_group$Freq / sum(tab_group$Freq))*100,2)
colnames(tab_group) = c("Group", "Count", "Perc")
tab_group
```

As we can see the classes are highly unbalanced. However we expected the 1st class (i.e. students and young-workers) to be the most populated, and the 4th one (i.e. elderly persons) to be the less populated.

At this point we want to analyzie the trends of trips over months and over the age groups:

```{r}
par(mfrow=c(1,2))

freq_month_19 = barplot(table(data_u85$month), 
     main = "Freq during months",
     xlab = "Months",
     ylab = "Freq",
     col = "light Blue")

mean_dur_19 = c()

for (val in 1:12){
  mean_dur_19[val] =  
    round(mean(tripdata_2019_r$tripduration[tripdata_2019_r$month == val])/60,2)
}

min_per_month_19 = barplot(mean_dur_19, 
     main = "Avg (min) per month",
     xlab = "Months",
     ylab = "Mins",
     ylim=c(0, max(mean_dur_19) + 5),
     col = "light green",
     names.arg = months_list)


```

From these plots we can see an increment on the number of trips (and also on the average duration) during the warmer months.

Now we check for the density distribution of the age groups over the months:

```{r}
par(mfrow=c(1,2))

freq_age_month = barplot(table(data_u85$age_groups, data_u85$month),
                         names.arg = months_list,
                         col = viridis(4))

legend("topleft", legend = c("1", "2", "3", "4"),
       fill = viridis(4),
       cex = 0.8)


freq_age_month = barplot(table(tripdata_2019_r$usertype, tripdata_2019_r$month),
                         names.arg=months_list,
                         beside = TRUE,
                         col = viridis(2))

legend("topleft", legend = c("0: Customer", "1:Subscriber"),
       fill = viridis(2),
       cex = 0.8)

```

```{r}
c1 = density(data_u85[data_u85$age_groups == 1,]$month, adjust = 3.5)
c2 = density(data_u85[data_u85$age_groups == 2,]$month, adjust = 3.5)
c3 = density(data_u85[data_u85$age_groups == 3,]$month, adjust = 3.5)
c4 = density(data_u85[data_u85$age_groups == 4,]$month, adjust = 3.5)

plot(c1, xlim = c(1,12), main = "Age density over months", col = our_palette[1], lwd = 2)
lines(c2, col = our_palette[2], lwd = 2)
lines(c3, col = our_palette[3], lwd = 2)
lines(c4, col = our_palette[4], lwd = 2)


legend("topleft", 
       legend = c("Group 1: 16-30", "Group 2: 31-45", "Group 3: 46-65", "Group 4: 65-85"),
       fill = our_palette)
```

### User analysis

```{r}
tab = table(data_u85$gender, data_u85$usertype)
hist = barplot(tab, 
        beside = TRUE, 
        legend = FALSE,
        main = "Gender and UserType",
        xlab = "UserType",
        ylab = "Freq",
        ylim=c(0, max(tab) + 100000),
        names.arg = c("Customer: 0", "Subscriber: 1"),
        col = viridis(3))

legend("topleft", legend = c("Unknown", "Male", "Female"),
       fill = viridis(3))

text(x = hist, y = tab + 50000, labels = tab, cex = .8)

```

BlueBikes seems to boast of loyal users.

```{r}
tab_sex = as.data.frame(table(data_u85$gender))
tab_sex$perc = round((tab_sex$Freq / sum(tab_sex$Freq))*100,2)
colnames(tab_sex) = c("User sex", "Count", "Perc")
tab_sex$`User sex`= c("0: Unknown", "1: Male", "2: Female")
tab_sex
```

The vast majority of the users seems to be Male. Even in this case we're facing unbalanced classes.

At this point we could check for differences in the trip duration among Customers and Subscribers

```{r}
EDA_UT = subset(data_u85, select = c(tripduration, usertype))

customer_dur = EDA_UT[EDA_UT$usertype == 0,]
subscriber_dur = EDA_UT[EDA_UT$usertype == 1,]

den_customer = density(log(customer_dur$tripduration))
den_subscriber = density(log(subscriber_dur$tripduration))

plot(den_customer, main="Trip duration: Customer vs Sub ", col="red")
lines(den_subscriber, col="green")

legend("topright", 
       legend = c("Customer", "Subscriber"),
       fill = c("red", "green"))


# Fill the areas
polygon(den_customer, col = rgb(0.9, 0, 0, alpha = 0.6))
polygon(den_subscriber, col = rgb(0, 0.9, 0, alpha = 0.6))

```

From this analysis we would have expected the opposite result: We had foreseen a longer average duration of the trips by the subscribers. By checking the monthly and the annual membership plans on the BlueBikes site however, we've seen that both include unlimited 45-min rides. At this point we could assume that subscribers are more prone to take more, but shorter, trips.

## **Correlations**

In this section we will use a tool named Correlation matrix in order to quantify the strength of the linear relationship between two variables. We are basically trying to observe the level of change in one variable due to the change in the other one.

```{r}
percs = get_perc(mode = "10k")
full = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

res = rcorr(as.matrix(data_train))
```

```{r}
corrplot(res$r,
         type = "upper",
         order = "hclust", 
         tl.col = "black",
         p.mat = res$P,
         sig.level = 0.01,
         insig = "blank",
         tl.cex = 0.8,
         number.cex = 0.6,
         diag = FALSE,
         method = "number",
         tl.srt = 45)
```

```{r}
#Pos Cor
corr_1<- ggplot(data_train, aes(startday, stopday)) + geom_jitter(color = "blue") + xlab("startday") + ylab("stopday")
corr_2<- ggplot(data_train, aes(starttime, stoptime)) + geom_jitter(color = "blue") + xlab("starttime") + ylab("stoptime")

grid.arrange(corr_1, corr_2, ncol = 2)
```

```{r}
dates <- vector(mode="character", length=nrow(data_u85))

for (i in 1:nrow(data_u85)){
  dates[i] = paste("2019", data_u85$month[i], data_u85$startday[i], sep="-")
}

dates <- weekdays(as.Date(dates))
data_u85$dayname = dates

dates <- vector(mode="character", length=nrow(data_o85))

for (i in 1:nrow(data_o85)){
  dates[i] = paste("2019", data_o85$month[i], data_o85$startday[i], sep="-")
}

dates <- weekdays(as.Date(dates))
data_o85$dayname = dates

tab = table(data_u85$dayname)
d = as.data.frame((tab))
tmp = d
d[1,] = tmp[3,]
d[2,] = tmp[4,]
d[3,] = tmp[5,]
d[4,] = tmp[2,]
d[5,] = tmp[7,]
d[7,] = tmp[1,]
tmp
barplot(d$Freq,
        names.arg = d$Var1,
        col = 'lightblue')
```
```{r}
d
tab = table(data_u85$age_groups,data_u85$dayname)
tmp = tab
tmp
tab[,1] = tmp[,3]
tab[,2] = tmp[,4]
tab[,3] = tmp[,5]
tab[,4] = tmp[,2]
tab[,5] = tmp[,7]
tab[,7] = tmp[,1]
tab

barplot(tab,
        names.arg = d$Var1,
        beside=FALSE,
        col = viridis(4))

legend("topright", c("1", "2", "3", "4"),
       fill = viridis(4), cex = 0.7)
```

### **Feature selection**

Feature selection is the process of reducing the number of input variables when developing a predictive model. This is really convenient when dealing with data science problems, which tipically consist of thousands of entries and several features. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model. The stepwise selection consists of iteratively adding and removing predictors, in the predictive model, in order to find the subset of variables in the data set resulting in the best performing model, that is a model that lowers prediction error.

There are three strategies of stepwise regression:

-   Forward selection: which starts with no predictors in the model, iteratively adds the most contributive predictors, and stops when the improvement is no longer statistically significant.

-   Backward selection: (or backward elimination), which starts with all predictors in the model (full model), iteratively removes the least contributive predictors, and stops when you have a model where all predictors are statistically significant.

-   Stepwise selection: which is a combination of forward and backward selections.

We've decided to perform the feature selection through a backward selection.

```{r}
####### BACKW FEATURE SELECTION ###################
percs = get_perc(mode = "50k")

full = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

labels_train = data_train$age_groups
labels_test = data_test$age_groups

data_train = subset(data_train, select = -c(age_groups))
data_test = subset(data_test, select = -c(age_groups))


# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
# run the RFE algorithm
results <- rfe(data_train, as.factor(labels_train), sizes=c(4:15), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))

#BEST FEATURE -> ALL - startday, stopday, bikeid (12)

```

This step has highlighted startday, stopday, bikeid as the least important feature. But they're not just not important: they degrade the accuracy of the model! For this reason, we're going to remove this feature when training our models.

```{r}
data_u85 = subset(data_u85, select = -c(startday, stopday, bikeid))
```

**Handling of unbalanced data**

As mentioned in the introduction, our dataset is highly unbalanced when considering the age groups. In order to build non-dummy models, we must act on the distribution of data. To overcome this problem we've build two functions, named "get_perc" and "get_balanced_v2": by using this function we're able to produce an almost balanced subsampled dataset. We used this subsample also to produce the correlation matrix, the feature selection and many other tests (also, without the subsampling we would not be able to produce this tests due to the huge amount of data).

```{r}
percs = get_perc(mode = "50k")
full = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = as.data.frame(full[1])

tab_group_balanced = as.data.frame(table((data_train$age_groups)))
tab_group_balanced$perc = round((tab_group_balanced$Freq / sum(tab_group_balanced$Freq))*100,2)
colnames(tab_group_balanced) = c("Group", "Count", "Perc")
tab_group_balanced
```

# MODELS

We tested several models, at the end of this section we will show the results among all of them:

-   Multinomial Logistic Regression
-   K-Nearest Neighborhood
-   Random Forest
-   Discriminant Analysis

### Multinomial Logistic Regression

The multinomial logistic regression is an extension of the logistic regression for multiclass classification tasks

```{r}
####### MULTINOMIAL LOGISTIC REGRESSION ###################
percs = get_perc(mode = "50k")

full = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = full[[1]]
data_test = full[[2]]

# Fit the model
multinom_age_model = multinom(age_groups ~., data = data_train)

pred = predict(multinom_age_model, newdata = data_train)
tab_train = table(pred, data_train$age_groups)

plot_stats(tab_train, mode="train", cf = F)

## TEST SET
pred = predict(multinom_age_model, newdata = data_test)
tab_test_mlr_age = table(pred, data_test$age_groups)

precision_mlr_age = round(precision(tab_test_mlr_age),2)
recall_mlr_age = round(recall(tab_test_mlr_age),2)

plot_stats(tab_test_mlr_age, cf = T)

```

### KNN

The K - nearest neighbors algorithm is a simple non-parametric *machine learning* method used for classification (but can be used also for regression). This algo predicts the outcome of a new observation based on the distance of k similar cases

```{r}
####################### KNN ################################

data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

#our Y
labels_train = data_train$age_groups
labels_test = data_test$age_groups

data_train = subset(data_train, select = -c(age_groups))
data_test = subset(data_test, select = -c(age_groups))

##run knn function
knn_model_pred = knn(data_train, data_test, cl=labels_train, k=10)

tab_test_knn = table(knn_model_pred, labels_test)

plot_stats(tab_test_knn, cf = T)
```

### Random Forest

The Random forest classifier is basically a set of decision trees (DT). This algo establishes the outcome based on the predictions of the decision trees. It predicts by taking the average or mean of the output from various trees.

```{r}
################### RANDOM FOREST ##########################

data_train = subset(data_train, select=-c(gender))
data_test = subset(data_test, select=-c(gender))

#our Y
labels_train = data_train$age_groups
labels_test = data_test$age_groups

data_train = subset(data_train, select = -c(age_groups))
data_test = subset(data_test, select = -c(age_groups))


rf_model = randomForest(x = data_train,
                        y = as.factor(labels_train),
                        ntree = 100,
                        xtest = data_train,
                        ytest = as.factor(labels_train),
                        keep.forest = TRUE)

rf_model_pred_train = predict(rf_model, newdata = data_train)
rf_model_pred = predict(rf_model, newdata = data_test)

# train resuls
tab_train_rf = table(rf_model_pred_train, labels_train)
plot_stats(tab_train_rf, mode="train", cf = F)
  
# test result
tab_test_rf_age = table(rf_model_pred, labels_test)
plot_stats(tab_test_rf_age, mode="test", cf = T)

precision_rf_age = round(precision(tab_test_rf_age),2)
recall_rf_age = round(recall(tab_test_rf_age),2)

```

```{r}
# Variable importance plot
varImpPlot(rf_model)
```

```{r}
rocs = multiclass.roc(as.numeric(labels_test), as.numeric(rf_model_pred))
rs = rocs[["rocs"]]
plot.roc(rs[[1]])
sapply(2:length(rs),function(i) lines.roc(rs[[i]],col=i))
```

## Discriminant Analysis

Discriminant analysis is used to predict the probability of belonging to a given class (or category) based on one or multiple predictor variables. Although both logistic regression and discriminant analysis can be used for binary classification tasks, the latter is more stable than the first one for multi-class classification problems. There are many methods for this analysis, in this project we focused on:

-   Linear discriminant analysis (LDA): Uses linear combinations of predictors to predict the class of a given observation.

-   Mixture discriminant analysis (MDA): Each class is assumed to be a Gaussian mixture of subclasses.

-   Quadratic discriminant analysis (QDA): More flexible than LDA. Here, there is no assumption that the covariance matrix of classes is the same.

-   Flexible Discriminant Analysis (FDA): Non-linear combinations of predictors is used.

```{r}
################### LDA ################################

data_train = full[[1]]
data_test = full[[2]]

# Fit the model
lda_model = lda(age_groups~., data = data_train)

pred = predict(lda_model, newdata = data_train)
tab_train = table(pred$class, data_train$age_groups)

plot_stats(tab_train, mode="train", cf = F)

## TEST SET
pred = predict(lda_model, newdata = data_test)
tab_test_lda_age = table(pred$class, data_test$age_groups)

precision_lda_age = round(precision(tab_test_lda_age),2)
recall_lda_age = round(recall(tab_test_lda_age),2)

plot_stats(tab_test_lda_age, cf = T)
```

```{r}
################## MDA ############################

mixture_model = mda(age_groups~., data = data_train)

pred_mda_train = mixture_model %>% predict(data_train)
tab_train_mda = table(pred_mda_train, data_train$age_groups)
plot_stats(tab_train_mda, mode="train", cf = F)

pred_mda_test = mixture_model %>% predict(data_test)
tab_test_mda_age = table(pred_mda_test, data_test$age_groups)

precision_mda_age = round(precision(tab_test_mda_age),2)
recall_mda_age = round(recall(tab_test_mda_age),2)

plot_stats(tab_test_mda_age, mode="test", cf = T)
```

```{r}
################## QDA ############################

quadratic_model = qda(age_groups~., data = data_train)

pred_qda_train = quadratic_model %>% predict(data_train)
tab_train_qda = table(pred_qda_train$class, data_train$age_groups)
plot_stats(tab_train_qda, mode="train", cf = F)

pred_qda_test = quadratic_model %>% predict(data_test)
tab_test_qda_age = table(pred_qda_test$class, data_test$age_groups)

precision_qda_age = round(precision(tab_test_qda_age),2)
recall_qda_age = round(recall(tab_test_qda_age),2)

plot_stats(tab_test_qda_age, mode="test", cf = T)
```

```{r}
################## FDA ############################

fda_model = fda(age_groups~., data = data_train)

pred_fda_train = fda_model %>% predict(data_train)
tab_train_fda = table(pred_fda_train, data_train$age_groups)
plot_stats(tab_train_fda, mode="train", cf = F)

pred_fda_test = fda_model %>% predict(data_test)
tab_test_fda_age = table(pred_fda_test, data_test$age_groups)

precision_fda_age = round(precision(tab_test_fda_age),2)
recall_fda_age = round(recall(tab_test_fda_age),2)

plot_stats(tab_test_fda_age, mode="test", cf = T)
```

## Age group prediction results

```{r}
########### SUMMARY TAB AGE MODELS ###########################

#vec_name_model = c("MLR", "KNN", "RF", "LDA", "MDA", "QDA", "FDA")
vec_name_model = c("MLR", "RF", "LDA", "MDA", "QDA", "FDA")

vec_acc_model = c((round(accuracy(tab_test_mlr_age),2)),
                  #(round(accuracy(tab_test_knn),2)),
                  (round(accuracy(tab_test_rf_age),2)),
                  (round(accuracy(tab_test_lda_age),2)),
                  (round(accuracy(tab_test_mda_age),2)),
                  (round(accuracy(tab_test_qda_age),2)),
                  (round(accuracy(tab_test_fda_age),2)))


tab_accuracy_models = data.frame(vec_name_model, vec_acc_model)

barplot_acc_model = barplot(tab_accuracy_models$vec_acc_model, 
     main = "Summary test accuracy AGE",
     xlab = "Model name",
     ylab = "Accuracy",
     col = "light green",
     names.arg = tab_accuracy_models$vec_name_model)

```

```{r}
precision_age= cbind(data.frame(precision_mlr_age), 
                      data.frame(precision_rf_age),
                      data.frame(precision_lda_age),
                      data.frame(precision_mda_age),
                      data.frame(precision_qda_age),
                      data.frame(precision_fda_age))

recall_age = cbind(data.frame(recall_mlr_age), 
                  data.frame(recall_rf_age),
                  data.frame(recall_lda_age),
                  data.frame(recall_mda_age),
                  data.frame(recall_qda_age),
                  data.frame(recall_fda_age))

par(mfrow=c(1, 2))

barplot_prec_model_age = barplot(as.matrix(precision_age), 
                          beside = T,
                          main = "Precision on AGE GROUPS",
                          xlab = "Model name",
                          ylab = "Precision",
                          names.arg = vec_name_model,
                          col = viridis(4))

legend("topright", c("1", "2", "3", "4"),
       fill = viridis(4), cex = 0.8)



barplot_rec_model_age = barplot(as.matrix(recall_age), 
                          beside = T,
                          main = "Recall on AGE GROUPS",
                          xlab = "Model name",
                          ylab = "Recall",
                          names.arg = vec_name_model,
                          col = viridis(4))

legend("topright", c("1", "2", "3", "4"),
       fill = viridis(4), cex = 0.8)
```

It is possible to see that the Random Forest is the most suitable model for our task. It provides both a quite good accuracy (compared with the other models), together with the highest values of precision and recall. However, we could try to improve the performance of this model via a majority voting ensemble, i.e. a model that combines the predictions from multiple other models.

### Majority-voting ensemble random forest

To build a solid meta-model, we first sampled a larger dataset and then we trained three different random forest with different params. The final predictions are based on a class-weighted majority voting system (to each class corresponds a certain weight which is inversely proportional to the number of data available for that class)

```{r}
####################### FULL TRAIN ######################################

percs = get_perc(mode = "50k")

one = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = as.data.frame(one[[1]])
data_test = as.data.frame(one[[2]])

# -----------------------------------------------------------------------

# RF 1
labels_train_one = data_train_one$age_groups
data_train = subset(data_train_one, select = -c(age_groups))

rf_model_one = randomForest(x = data_train,
                            y = as.factor(labels_train),
                            ntree = 150,
                            xtest = data_train,
                            ytest = as.factor(labels_train),
                            keep.forest = TRUE)

# -----------------------------------------------------------------------

# RF 2
labels_train = data_train$age_groups
data_train = subset(data_train, select = -c(age_groups))

rf_model_two = randomForest(x = data_train,
                            y = as.factor(labels_train),
                            ntree = 100,
                            xtest = data_train,
                            ytest = as.factor(labels_train),
                            keep.forest = TRUE)

# -----------------------------------------------------------------------

# RF 3
labels_train_three = data_train_three$age_groups
data_train_three = subset(data_train_three, select = -c(age_groups))

rf_model_three = randomForest(x = data_train_three,
                              y = as.factor(labels_train_three),
                              ntree = 50)

```

```{r}

# --------------- LOAD TRAINED MODELS -------------- #
rf_100 = readRDS(load_dataset("models","rf_best.rda"))
rf_50 = readRDS(load_dataset("models", "rf_best_g_50.rda"))
rf_10 = readRDS(load_dataset("models", "rf_best_g_10.rda"))
# ------------------------------------------------- #


####### PREDICT TEST #########
pred_1 = predict(rf_100, newdata = subset(data_test, select = -c(age_groups)))
pred_2 = predict(rf_50, newdata = subset(data_test, select = -c(age_groups)))
pred_3 = predict(rf_10, newdata = subset(data_test, select = -c(age_groups)))

a = as.data.frame(pred_1)
colnames(a) = c("pred_a")
a$pred_a = as.numeric(a$pred_a)

b = as.data.frame(pred_2)
colnames(b) = c("pred_b")
b$pred_b = as.numeric(b$pred_b)

c = as.data.frame(pred_3)
colnames(c) = c("pred_c")
c$pred_c = as.numeric(c$pred_c)

a$pred_b = (b$pred_b)
a$pred_c = (c$pred_c)
a$pred_a = (a$pred_a)
rm(b,c)

pred_1a = (a$pred_a)
pred_2a = (a$pred_b)
pred_3a = (a$pred_c)

df_prob_1 = as.data.frame(predict(rf_100, newdata=subset(data_test, select = -c(age_groups)), type='prob'))
df_prob_2 = as.data.frame(predict(rf_50, newdata=subset(data_test, select = -c(age_groups)), type='prob'))
df_prob_3 = as.data.frame(predict(rf_10, newdata=subset(data_test, select = -c(age_groups)), type='prob'))

final = vector(mode="list", length = length(pred_1a))
w_class = c(0.48, 0.72, 0.79, 0.98)
c = 0

ty <- function(num, pred){

  if(length(num) != 1){
    return(pred)
  }else{
    return(num)
  }
}

for (i in (1:length(final))){
  if(pred_1a[i] == pred_2a[i] | pred_1a[i] == pred_3a[i] | pred_3a[i] == pred_2a[i]){
    final[i] = pred_1a[i]
  }else{
    prob_pred_1 = df_prob_1[i,]
    prob_pred_2 = df_prob_2[i,]
    prob_pred_3 = df_prob_3[i,]

    m1 = max(prob_pred_1)
    m2 = max(prob_pred_2)
    m3 = max(prob_pred_3)

    pos1 = which(prob_pred_1 == m1)
    pos2 = which(prob_pred_2 == m2)
    pos3 = which(prob_pred_3 == m3)
    
    pos1 = ty(pos1, pred_1a[i])
    pos2 = ty(pos2, pred_2a[i])
    pos3 = ty(pos3, pred_3a[i])

    val1 = (as.numeric(w_class[pos1]) * m1)
    val2 = (as.numeric(w_class[pos2]) * m2)
    val3 = (as.numeric(w_class[pos3]) * m3)
    
    fin = max(val1, val2, val3)
    if(fin == val1){
      pos = pos1
    }

    if(fin == val2){
      pos = pos2
    }
    
    if(fin == val3){
      pos = pos3
    }
    final[i] = 1
    c = c+1
  }
}

a$fin = as.numeric(final)

# test result
result_ensemble = table(a$fin, data_test$age_groups)
plot_stats(result_ensemble, cf = T)
```

```{r}
cool_cf(a$fin, data_test$age_groups)
```

```{r}
precision_enseble = round(precision(result_ensemble),2)
recall_ensemble = round(recall(result_ensemble),2)

par(mfrow=c(1, 2))

barplot_prec_ensemble = barplot(as.matrix(precision_enseble), 
                          beside = T,
                          main = "Precision",
                          xlab = "Ensemble",
                          ylab = "Precision",
                          col = viridis(4))

legend("topright", c("1", "2", "3", "4"),
       fill = viridis(4), cex = 0.8)


barplot_rec_ensemble = barplot(as.matrix(recall_ensemble), 
                          beside = T,
                          main = "Recall",
                          xlab = "Ensemble",
                          ylab = "Recall",
                          col = viridis(4))

legend("topright", c("1", "2", "3", "4"),
       fill = viridis(4), cex = 0.8)

```

The esemble shreds the results of all previous models!

|              |           |
|:------------:|:---------:|
| **Accuracy** | **86.05** |

| Metric    | Group 1 | Group 2 | Group 3 | Group 4 |
|-----------|---------|---------|---------|---------|
| Precision | 75.6    | 88.98   | 97.44   | 98.91   |
| Recall    | 96.69   | 83.92   | 81.24   | 64.7    |

This model will be used to predict the age groups of the 2020 users.

# GENDER

To predict the gender of a user, we used the same workflow shown in the age group section. Here's a brief recap:

-   Backward feature selection (the features removed are the same of the previous analysis)

-   Models (GLM, RF, KNN, Discriminant Analysis)

```{r}
####### BACKW FEATURE SELECTION ###################

percs = get_perc(mode = "50k")

full = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

labels_train = data_train$gender
labels_test = data_test$gender

data_train = subset(data_train, select = -c(gender))
data_test = subset(data_test, select = -c(gender))


# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
# run the RFE algorithm
results <- rfe(data_train, as.factor(labels_train), sizes=c(4:15), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
```

### GLM

Since in this scenario we're considering just the classification wrt the gender, rather than using a multinomial logisic regression, we're using a simple glm

```{r}
###################### GLM #######################

percs = get_perc(mode = "50k")

full = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

data_train$gender[data_train$gender == 1] = 0
data_train$gender[data_train$gender == 2] = 1

data_test$gender[data_test$gender == 1] = 0
data_test$gender[data_test$gender == 2] = 1

# Fit the model
glm_model_gender = glm(gender ~., data = data_train, family = binomial)

#Converting the prediction in {0,1} according to the chosen threshold:
pred_glm_gender = predict(glm_model_gender, data_test, type = "response")
pred_glm_compl = ifelse(pred_glm_gender > 0.6, 1, 0)


tab_test_glm_gen = table(pred_glm_compl, data_test$gender)
precision_glm_gen = round(precision(tab_test_glm_gen),2)
recall_glm_gen = round(recall(tab_test_glm_gen),2)

plot_stats(tab_test_glm_gen, mode="test", cf = T)
```

```{r}
summary(glm_model_gender)
```

```{r}
########################## RAND FOREST #################################

data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

#our Y
labels_train = data_train$gender
labels_test = data_test$gender

data_train = subset(data_train, select = -c(gender))
data_test = subset(data_test, select = -c(gender))

rf_model = randomForest(x = data_train,
                        y = as.factor(labels_train),
                        ntree = 100)

rf_model_pred_train = predict(rf_model, newdata = data_train)
rf_model_pred = predict(rf_model, newdata = data_test)

# train resuls
tab_train_rf = table(rf_model_pred_train, labels_train)
plot_stats(tab_train_rf, mode="train", cf = F)
  
# test result
tab_test_rf_gen = table(rf_model_pred, labels_test)
plot_stats(tab_test_rf_gen, mode="test", cf = T)

precision_rf_gen = round(precision(tab_test_rf_gen),2)
recall_rf_gen = round(recall(tab_test_rf_gen),2)

saveRDS(rf_model, file='rf_for_gender.rda')

```

```{r}
####################### KNN ################################

data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

#our Y
labels_train = data_train$gender
labels_test = data_test$gender

data_train = subset(data_train, select = -c(gender))
data_test = subset(data_test, select = -c(gender))

##run knn function
knn_model_pred = knn(data_train, data_test, cl=labels_train, k=10)

tab_test_knn = table(knn_model_pred, labels_test)

plot_stats(tab_test_knn, cf = T)

```

```{r}
################### LDA ################################

data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

# Fit the model
lda_model = lda(gender~., data = data_train)

pred = predict(lda_model, newdata = data_train)
tab_train = table(pred$class, data_train$gender)

plot_stats(tab_train, mode="train", cf = F)

## TEST SET
pred = predict(lda_model, newdata = data_test)
tab_test_lda_gen = table(pred$class, data_test$gender)

precision_lda_gen = round(precision(tab_test_lda_gen),2)
recall_lda_gen = round(recall(tab_test_lda_gen),2)

plot_stats(tab_test_lda_gen, cf = T)

```

```{r}
################### QDA ################################

# Fit the model
qda_model = qda(gender~., data = data_train)

pred = predict(qda_model, newdata = data_train)
tab_train = table(pred$class, data_train$gender)

plot_stats(tab_train, mode="train", cf = F)

## TEST SET
pred = predict(qda_model, newdata = data_test)
tab_test_qda_gen = table(pred$class, data_test$gender)

precision_qda_gen = round(precision(tab_test_qda_gen),2)
recall_qda_gen = round(recall(tab_test_qda_gen),2)

plot_stats(tab_test_qda_gen, cf = T)

```

```{r}
################### MDA ################################

# Fit the model
mda_model = mda(gender~., data = data_train)

pred = predict(mda_model, newdata = data_train)
tab_train = table(pred, data_train$gender) 

plot_stats(tab_train, mode="train", cf = F)

## TEST SET
pred = predict(mda_model, newdata = data_test)
tab_test_mda_gen = table(pred, data_test$gender)

precision_mda_gen = round(precision(tab_test_mda_gen),2)
recall_mda_gen = round(recall(tab_test_mda_gen),2)

plot_stats(tab_test_mda_gen, cf = T)

```

```{r}
################### FDA ################################

# Fit the model
fda_model = fda(gender~., data = data_train)

pred = predict(fda_model, newdata = data_train)
tab_train = table(pred, data_train$gender) 

plot_stats(tab_train, mode="train", cf = F)

## TEST SET
pred = predict(fda_model, newdata = data_test)
tab_test_fda_gen = table(pred, data_test$gender)

precision_fda_gen = round(precision(tab_test_fda_gen),2)
recall_fda_gen = round(recall(tab_test_fda_gen),2)

plot_stats(tab_test_fda_gen, cf = T)
```

## Gender prediction results

As previously done with age groups, here we present the results obtained by each model, considering both the accuracy and the precision/recall.

```{r}
########### SUMMARY TAB AGE MODELS ###########################

#vec_name_model = c("GLM", "KNN", "RF", "LDA", "MDA", "QDA")
vec_name_model = c("GLM", "RF", "LDA", "MDA", "QDA", "FDA")

vec_acc_model_gen = c((round(accuracy(tab_test_glm_gen),2)),
                      #(round(accuracy(tab_test_knn),2)),
                      (round(accuracy(tab_test_rf_gen),2)),
                      (round(accuracy(tab_test_lda_gen),2)),
                      (round(accuracy(tab_test_mda_gen),2)),
                      (round(accuracy(tab_test_qda_gen),2)),
                      (round(accuracy(tab_test_fda_gen),2)))


tab_accuracy_models = data.frame(vec_name_model, vec_acc_model_gen)

barplot_acc_model = barplot(tab_accuracy_models$vec_acc_model_gen, 
                             main = "Summary test accuracy GENDER",
                             xlab = "Model name",
                             ylab = "Accuracy",
                             col = "light blue",
                             names.arg = tab_accuracy_models$vec_name_model)

```

Excluding the good result obtained by the random forest, we would have expected a little higher values from the other models, considering the fact that we moved from a multiclass classification problem (4 classes in age groups) to a binomial one (gender m/f).

```{r}

precision_gen= cbind(data.frame(precision_glm_gen), 
                      data.frame(precision_rf_gen),
                      data.frame(precision_lda_gen),
                      data.frame(precision_mda_gen),
                      data.frame(precision_qda_gen),
                      data.frame(precision_fda_gen))

recall_gen = cbind(data.frame(recall_glm_gen), 
                  data.frame(recall_rf_gen),
                  data.frame(recall_lda_gen),
                  data.frame(recall_mda_gen),
                  data.frame(recall_qda_gen),
                  data.frame(recall_fda_gen))

par(mfrow=c(1,2))

barplot_prec_model = barplot(as.matrix(precision_gen), 
                          beside = T,
                          main = "Precision on GENDER",
                          xlab = "Model name",
                          ylab = "Precision",
                          names.arg = vec_name_model,
                          col = viridis(2))

legend("topright", c("Male", "Female"),
       fill = viridis(2), cex = 0.8)



barplot_rec_model = barplot(as.matrix(recall_gen), 
                          beside = T,
                          main = "Recall on GENDER",
                          xlab = "Model name",
                          ylab = "Recall",
                          names.arg = vec_name_model,
                          col = viridis(2))

legend("topright", c("Male", "Female"),
       fill = viridis(2), cex = 0.8)
```

In this case a simple random forest outperform all the other methods:

| **Accuracy** | **71.48** |
|:------------:|:---------:|

| Metric    | Male  | Female |
|-----------|-------|--------|
| Precision | 72.65 | 69.44  |
| Recall    | 80.55 | 59.31  |

In this case we consider the results obtained sufficiently high, thus we will not proceed with the construction of another meta-model. This model will be used to predict the gender of a user in the 2020.

# MAPS

As mentioned in the introduction, the year 2020 brought changes that no one would have expected. This caused significant changes even in the habits of all of us. To compare some of the differences between the pre-pandemic and the post-pandemic users, we will we will dive into a deep exploration of the data related to the BlueBikes stations: their locations, the most used and also the most common path divided by age groups. In order to be able to visualize these informations in the most suitable way, we will use a combination of R and some scripts in Python: thanks to the Folium library we will be able to generate Boston's maps, pointing out all the points of interest, along with user's favorite paths.

Here we first generate the tables with the most used stations, both start and end ones.

```{r}
data_u85 = tripdata_2019_r[tripdata_2019_r$age <= 85,]
data_u85$age_groups = cut(data_u85$age, breaks=split_ages, include.lowest = TRUE, labels = FALSE)
data_u85 = subset(data_u85, select = -c(age, birth.year, year))

data = subset(data_u85, select = c(age_groups,
                                  start.station.name,
                                  start.station.latitude, 
                                  start.station.longitude,
                                  end.station.name,
                                  end.station.latitude, 
                                  end.station.longitude))

EDA_id_station = subset(data, select = c(start.station.name,
                                         end.station.name,
                                         age_groups))

# TOP 10 START #
tab = as.data.frame(table(EDA_id_station$start.station.name))
top_10_start = tab[order(tab$Freq, decreasing = T),][1:10,]
colnames(top_10_start) = c("start_name", "Freq")

start_tab = data.frame(matrix(NA, nrow = nrow(top_10_start), ncol = 4))
colnames(start_tab) = c("1","2", "3", "4")

for (rowIdx in 1:(nrow(start_tab))) {
  for (colIdx in 1:(ncol(start_tab))) {
    start_tab[rowIdx, colIdx] = nrow(subset(EDA_id_station, age_groups == colIdx & start.station.name == top_10_start[rowIdx,1]))
  }
}
start_tab$start_name = top_10_start$start_name


# TOP 10 END #
end_tab = as.data.frame(table(EDA_id_station$end.station.name))
top_10_end = tab[order(end_tab$Freq, decreasing = T),][1:10,]
colnames(top_10_end) = c("end_name", "Freq")

end_tab = data.frame(matrix(NA, nrow = nrow(top_10_end), ncol = 4))
colnames(end_tab) = c("1","2", "3", "4")

for (rowIdx in 1:(nrow(end_tab))) {
  for (colIdx in 1:(ncol(end_tab))) {
    end_tab[rowIdx, colIdx] = nrow(subset(EDA_id_station, age_groups == colIdx & end.station.name == top_10_end[rowIdx,1]))
  }
}

end_tab$end_name = top_10_end$end_name
```

```{r}
head(start_tab)
```

```{r}
head(end_tab)
```

```{r}
bar = barplot(as.matrix(subset(start_tab, select = -c(start_name))), 
        beside = T,
        cex.names = 1,
        horiz = T,
        las=1,
        main = "TOP-10 Most used start station by age",
        space = c(0,3),
        col = viridis(nrow(start_tab)))

legend("topright", legend = start_tab$start_name,
       fill = viridis(nrow(start_tab)), cex = 0.8)

```

```{r}
bar = barplot(as.matrix(subset(end_tab, select = -c(end_name))), 
        beside = T,
        cex.names = 1,
        horiz = T,
        las=1,
        main = "TOP-10 Most used end station by age",
        space = c(0,3),
        col = viridis(nrow(start_tab)))

legend("topright", legend = end_tab$end_name,
       fill = viridis(nrow(end_tab)), cex = 0.8)
```

We now save this informations in a .csv file in order to use it in the Python script

```{r}
map_start = subset(data_u85, select = c(start.station.name,
                                  start.station.latitude, 
                                  start.station.longitude))
map_start = map_start[!duplicated(map_start[ , c("start.station.name")]),]
map_start = map_start[map_start$start.station.name %in% start_tab$start_name,]


map_end = subset(data_u85, select = c(end.station.name,
                                  end.station.latitude, 
                                  end.station.longitude))
map_end = map_end[!duplicated(map_end[ , c("end.station.name")]),]
map_end = map_end[map_end$end.station.name %in% end_tab$end_name,]


pos_19 = cbind(map_start, 
               end.station.name = map_end$end.station.name,
               end.station.latitude = map_end$end.station.latitude,
               end.station.longitude = map_end$end.station.longitude)

write.csv(pos_19, paste(getwd(), "SL_dataset/2019_pos.csv", sep = "/"), row.names = FALSE)
```

The most common path divided by age groups

```{r}
## ------------------------ TOP 10 TRIP BY AGE ----------------------------- ##
data_top = data_u85[data_u85$start.station.name %in% start_tab$start_name,]
data_top = data_top[data_top$end.station.name %in% end_tab$end_name,]

tab = table(data_top$start.station.name, data_top$end.station.name, data_top$age_groups)
tab = as.data.frame(tab)


colnames(tab) = c('start_name', 'end_name', 'age', 'freq')
for (idx in 1:nrow(tab)){
  tab$start_longitude[idx] = data_top$start.station.longitude[data_top$start.station.name == tab$start_name[idx]][1]
  tab$start_latitude[idx] = data_top$start.station.latitude[data_top$start.station.name == tab$start_name[idx]][1]
  tab$end_longitude[idx] = data_top$end.station.longitude[data_top$end.station.name == tab$end_name[idx]][1]
  tab$end_latitude[idx] = data_top$end.station.latitude[data_top$end.station.name == tab$end_name[idx]][1]
}

tmp = tab[tab$age==1,]
out = tmp[order(tmp$freq, decreasing = T),][1:10,]
for (i in 2:4){
  print(i)
  tmp = tab[tab$age==i,]
  out = rbind(out, tmp[order(tmp$freq, decreasing = T),][1:10,])
}

write.csv(out, paste(getwd(), "SL_dataset/trips_age_19.csv", sep = "/"), row.names = FALSE)
```

### Map creation via Python script

```{python}
import pandas as pd
import numpy as np
import folium
import webbrowser

df_acc = pd.read_csv('C:/Users/Utente/Desktop/SL_project/SL_dataset/trips_age_19.csv', dtype=object)

df_acc['start_name'] = df_acc['start_name'].astype(str)
df_acc['end_name'] = df_acc['end_name'].astype(str)

df_acc['age'] = df_acc['age'].astype(int)
df_acc['freq'] = df_acc['freq'].astype(int)

df_acc['start_longitude'] = df_acc['start_longitude'].astype(float)
df_acc['start_latitude'] = df_acc['start_latitude'].astype(float)

df_acc['end_longitude'] = df_acc['end_longitude'].astype(float)
df_acc['end_latitude'] = df_acc['end_latitude'].astype(float)

for i in range(1,2):
  print(i)
  map_hooray = folium.Map(location=[42.361145, -71.057083], zoom_start = 12) 
  feature = "TripStart_" + str(i)
  
  fil = df_acc[df_acc['age'] == i]
  feature_group = folium.FeatureGroup(feature)
  
  s_lat = fil['start_latitude']
  s_lng = fil['start_longitude']
  s_name = fil['start_name']
  
  e_lat = fil['end_latitude']
  e_lng = fil['end_longitude']
  e_name = fil['end_name']
  
  
  for s_lat, s_lng, s_name, e_lat, e_lng, e_name in zip(s_lat, s_lng, s_name, e_lat, e_lng, e_name):
    if s_lat == e_lat and s_lng == e_lng:
      s_lng = s_lng + 0.001
    feature_group.add_child(folium.Marker(location=[s_lat,s_lng], popup=s_name, icon=folium.Icon(color="blue")))
    feature_group.add_child(folium.Marker(location=[e_lat,e_lng], popup=e_name, icon=folium.Icon(color="orange")))
    
    folium.PolyLine( [[s_lat, s_lng],[e_lat, e_lng]] ).add_to(map_hooray)
      

  map_hooray.add_child(feature_group)
  map_hooray.save("test_" + str(i) + ".html")
```

```{python}
import pandas as pd
import numpy as np
import folium
import webbrowser

df_acc = pd.read_csv('C:/Users/Utente/Desktop/SL_project/SL_dataset/2019_pos.csv', dtype=object)

# ----------------------------- START ----------------------------------------- #
map_hooray = folium.Map(location=[42.361145, -71.057083], zoom_start = 12) 
df_acc['start.station.name'] = df_acc['start.station.name'].astype(str)
df_acc['start.station.latitude'] = df_acc['start.station.latitude'].astype(float)
df_acc['start.station.longitude'] = df_acc['start.station.longitude'].astype(float)


feature_group = folium.FeatureGroup("LocationsStart")
lat = df_acc['start.station.latitude']
lng = df_acc['start.station.longitude']
name = df_acc['start.station.name']

for lt, lg, nm in zip(lat, lng, name):
    feature_group.add_child(folium.Marker(location=[lt,lg],popup=nm,icon=folium.Icon(color="blue")))

map_hooray.add_child(feature_group)
map_hooray.save("BostonMap_top10_start_19.html")
# ----------------------------- START ----------------------------------------- # 


# ----------------------------- END ----------------------------------------- #
map_hooray = folium.Map(location=[42.361145, -71.057083], zoom_start = 12) 
df_acc['end.station.name'] = df_acc['end.station.name'].astype(str)
df_acc['end.station.latitude'] = df_acc['end.station.latitude'].astype(float)
df_acc['end.station.longitude'] = df_acc['end.station.longitude'].astype(float)


feature_group = folium.FeatureGroup("LocationsStart")
lat = df_acc['end.station.latitude']
lng = df_acc['end.station.longitude']
name = df_acc['end.station.name']

for lt, lg, nm in zip(lat, lng, name):
    feature_group.add_child(folium.Marker(location=[lt,lg],popup=nm,icon=folium.Icon(color="orange")))

map_hooray.add_child(feature_group)
map_hooray.save("BostonMap_top10_end_19.html")
# ----------------------------- END ----------------------------------------- #
```

```{r}
tab = as.data.frame(table(data$age_groups, data$start.station.name))
colnames(tab) = c("age_groups", "start.station.name", "freq")

one = tab[tab$age_groups == 1,]
temp = one[order(one$freq, decreasing = T),][1:10,]

two = tab[tab$age_groups == 2,]
temp = rbind(temp, (two[order(two$freq, decreasing = T),][1:10,]))

three = tab[tab$age_groups == 3,]
temp = rbind(temp, (three[order(three$freq, decreasing = T),][1:10,]))

four = tab[tab$age_groups == 4,]
temp = rbind(temp, (four[order(four$freq, decreasing = T),][1:10,]))

temp$lat = NA
temp$long = NA
temp = as.data.frame(temp)

for (idx in 1:nrow(temp)){
  name = temp$start.station.name[idx]
  
  temp[idx, 4] = data$start.station.latitude[data$start.station.name == name][1]
  temp[idx, 5] = data$start.station.longitude[data$start.station.name == name][1]
}


write.csv(temp, paste(getwd(), "SL_dataset/start_station_by_age_19.csv", sep = "/"), row.names = FALSE)
```

```{python}
df_acc = pd.read_csv('C:/Users/Utente/Desktop/SL_project/SL_dataset/start_station_by_age_19.csv', dtype=object)

df_acc['start.station.name'] = df_acc['start.station.name'].astype(str)
df_acc['age_groups'] = df_acc['age_groups'].astype(int)
df_acc['freq'] = df_acc['freq'].astype(int)
df_acc['lat'] = df_acc['lat'].astype(float)
df_acc['long'] = df_acc['long'].astype(float)


for i in range(1,5):
  map_hooray = folium.Map(location=[42.361145, -71.057083], zoom_start = 12) 
  feature = "LocationsStart_" + str(i)
  
  
  fil = df_acc[df_acc['age_groups'] == i]
  feature_group = folium.FeatureGroup(feature)
  lat = fil['lat']
  lng = fil['long']
  name = fil['start.station.name']
  
  
  for lt, lg, nm in zip(lat, lng, name):
      feature_group.add_child(folium.Marker(location=[lt,lg],popup=nm,icon=folium.Icon(color="blue")))
  
  
  map_hooray.add_child(feature_group)
  map_hooray.save("BostonMap_age" + str(i) + ".html")
```

## Map visualization

## Comparison 2019 2020

```{r}
par(mfrow=c(1,2))

barplot_age_group = barplot(table(data_u85$age_groups), 
     main = "Age groups count 2019",
     xlab = "Age group",
     ylab = "Freq",
     col = viridis(4))

legend("topright", 
       legend = c("Group 1: 16-30", 
                  "Group 2: 31-45", 
                  "Group 3: 46-65", 
                  "Group 4: 65-85"),
       fill = viridis(4))

barplot_age_group = barplot(table(data_2020$age_groups), 
     main = "Age groups count 2020",
     xlab = "Age group",
     ylab = "Freq",
     col = viridis(4))

legend("topright", 
       legend = c("Group 1: 16-30", 
                  "Group 2: 31-45", 
                  "Group 3: 46-65", 
                  "Group 4: 65-85"),
       fill = viridis(4))
```


```{r}
par(mfrow=c(1,2))

freq_month_19 = hist(tripdata_2019_r$month,
     breaks = 12,
     main = "Freq during months 2019",
     xlab = "Months",
     ylab = "Freq",
     col = "light Blue",
     freq = FALSE)

lines(density(tripdata_2019_r$month, adjust = 4), # density plot
 lwd = 2, # thickness of line
 col = "chocolate3")

freq_month_20 = hist(tripdata_2020_r$month, 
     main = "Freq during months 2020",
     xlab = "Months",
     ylab = "Freq",
     col = "light Blue",
     freq = FALSE,
     breaks = 12)

lines(density(tripdata_2020_r$month, adjust = 3.5), # density plot
 lwd = 2, # thickness of line
 col = "chocolate3")
```

```{r}
par(mfrow=c(1,2))

freq_age_month = barplot(table(data_u85$age_groups, data_u85$month),
                         names.arg = months_list,
                         col = viridis(4),
                         main='Frequency during months 2019')

legend("topleft", legend = c("1", "2", "3", "4"),
       fill = viridis(4),
       cex = 0.8)


freq_age_month = barplot(table(data_2020$age_groups, data$month),
                         names.arg = months_list,
                         col = viridis(4),
                         main='Frequency during months 2020')

```

```{r}
par(mfrow=c(1,2))
freq_age_month = barplot(table(tripdata_2019_r$usertype, tripdata_2019_r$month),
                         names.arg=months_list,
                         beside = TRUE,
                         col = viridis(2),
                         main='Frequency during months 2019')

legend("topleft", legend = c("0: Customer", "1:Subscriber"),
       fill = viridis(2),
       cex = 0.8)

freq_age_month = barplot(table(tripdata_2020_r$usertype, tripdata_2020_r$month),
                         names.arg=months_list,
                         beside = TRUE,
                         col = viridis(2),
                         main='Frequency during months 2020')

```

```{r}
par(mfrow=c(1,2))
c1 = density(data_u85[data_u85$age_groups == 1,]$month, adjust = 3.5)
c2 = density(data_u85[data_u85$age_groups == 2,]$month, adjust = 3.5)
c3 = density(data_u85[data_u85$age_groups == 3,]$month, adjust = 3.5)
c4 = density(data_u85[data_u85$age_groups == 4,]$month, adjust = 3.5)

plot(c1, xlim = c(1,12), main = "Age density over months", col = our_palette[1], lwd = 2)
lines(c2, col = our_palette[2], lwd = 2)
lines(c3, col = our_palette[3], lwd = 2)
lines(c4, col = our_palette[4], lwd = 2)


legend("topleft", 
       legend = c("Group 1: 16-30", "Group 2: 31-45", "Group 3: 46-65", "Group 4: 65-85"),
       fill = our_palette,
       cex = 0.8)

c1 = density(data_u85[data_2020$age_groups == 1,]$month, adjust = 3.5)
c2 = density(data_u85[data_2020$age_groups == 2,]$month, adjust = 3.5)
c3 = density(data_u85[data_2020$age_groups == 3,]$month, adjust = 3.5)
c4 = density(data_u85[data_2020$age_groups == 4,]$month, adjust = 3.5)

plot(c1, xlim = c(1,12), main = "Age density over months", col = our_palette[1], lwd = 2)
lines(c2, col = our_palette[2], lwd = 2)
lines(c3, col = our_palette[3], lwd = 2)
lines(c4, col = our_palette[4], lwd = 2)


```

```{r}
par(mfrow=c(1,2))

tab = table(data_u85$gender, data_u85$usertype)
hist = barplot(tab, 
        beside = TRUE, 
        legend = FALSE,
        main = "Gender and UserType",
        xlab = "UserType",
        ylab = "Freq",
        ylim=c(0, max(tab) + 100000),
        names.arg = c("Customer: 0", "Subscriber: 1"),
        col = viridis(3))

legend("topleft", legend = c("Unknown", "Male", "Female"),
       fill = viridis(3))

text(x = hist, y = tab + 50000, labels = tab, cex = .8)

tab = table(data_2020$gender, data$usertype)
hist = barplot(tab, 
        beside = TRUE, 
        legend = FALSE,
        main = "Gender and UserType 2020",
        xlab = "UserType",
        ylab = "Freq",
        ylim=c(0, max(tab) + 100000),
        names.arg = c("Customer: 0", "Subscriber: 1"),
        col = viridis(3))


text(x = hist, y = tab + 50000, labels = tab, cex = .8)
```
```{r}
par(mfrow=c(1,2))

EDA_UT = subset(tripdata_2019_r, select = c(tripduration, usertype))

customer_dur = EDA_UT[EDA_UT$usertype == 0,]
subscriber_dur = EDA_UT[EDA_UT$usertype == 1,]

den_customer = density(log(customer_dur$tripduration))
den_subscriber = density(log(subscriber_dur$tripduration))

plot(den_customer, main="Customer vs Sub trip duration", col=our_palette[3])
lines(den_subscriber, col=our_palette[3])

abline(v=mean(log(customer_dur$tripduration)),
       col = our_palette[3],
       lwd =3)
abline(v=mean(log(subscriber_dur$tripduration)),
       col = our_palette[4],
       lwd =3)

legend("topright", 
       legend = c("Customer", "Subscriber"),
       fill = c(our_palette[3], our_palette[4]))
 

# Fill the areas
polygon(den_customer, col = rgb(0, 0, 1, alpha = 0.6))
polygon(den_subscriber, col = rgb(1, 0.5, 0, alpha = 0.6))

x = seq(4, 12, by = .1)
y = dnorm(x, mean = mean(log(customer_dur$tripduration)), sd = sd(log(customer_dur$tripduration)))
lines(x,y, type='l',
      lwd=3,
      col = our_palette[3])

x = seq(4, 12, by = .1)
y = dnorm(x, mean = mean(log(subscriber_dur$tripduration)), sd = sd(log(subscriber_dur$tripduration)))
lines(x,y, type='l',
      lwd=3,
      col = our_palette[4])

##################################################

EDA_UT = subset(tripdata_2020_r, select = c(tripduration, usertype))

customer_dur = EDA_UT[EDA_UT$usertype == 0,]
subscriber_dur = EDA_UT[EDA_UT$usertype == 1,]

den_customer = density(log(customer_dur$tripduration))
den_subscriber = density(log(subscriber_dur$tripduration))

plot(den_customer, main="Customer vs Sub trip duration", col=our_palette[3])
lines(den_subscriber, col=our_palette[3])

abline(v=mean(log(customer_dur$tripduration)),
       col = our_palette[3],
       lwd =3)
abline(v=mean(log(subscriber_dur$tripduration)),
       col = our_palette[4],
       lwd =3)
 

# Fill the areas
polygon(den_customer, col = rgb(0, 0, 1, alpha = 0.6))
polygon(den_subscriber, col = rgb(1, 0.5, 0, alpha = 0.6))

x = seq(4, 12, by = .1)
y = dnorm(x, mean = mean(log(customer_dur$tripduration)), sd = sd(log(customer_dur$tripduration)))
lines(x,y, type='l',
      lwd=3,
      col = our_palette[3])

x = seq(4, 12, by = .1)
y = dnorm(x, mean = mean(log(subscriber_dur$tripduration)), sd = sd(log(subscriber_dur$tripduration)))
lines(x,y, type='l',
      lwd=3,
      col = our_palette[4])
```

```{r}
par(mfrow=c(1,2))

t=table(tripdata_2019_r$usertype, tripdata_2019_r$month)
s=colSums(t)
for (i in 1:12){
  t[2*i-1] = t[2*i-1]/s[i]
  t[2*i] = t[2*i]/s[i]
}

barplot(t,
        main='Usertype fraction per month 2019',
        xlab='Month',
        ylab='Fraction',
        col=viridis(2),
        legend=TRUE,
        args.legend = list(legend=(c('Subscriber', 'Customer'))))

t=table(tripdata_2020_r$usertype, tripdata_2020_r$month)
s=colSums(t)
for (i in 1:12){
  t[2*i-1] = t[2*i-1]/s[i]
  t[2*i] = t[2*i]/s[i]
}

barplot(t,
        main='Usertype fraction per month 2020',
        xlab='Month',
        ylab='Fraction',
        col=viridis(2),
        args.legend = list(legend=(c('Subscriber', 'Customer'))))
```

