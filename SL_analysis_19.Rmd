---
title: "SL_proj_doc"
author: "Nicola Cassetta, Andrea di Trani"
date: '2022-09-11'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Project Introduction**

*BlueBikes* [] is a public bike sharing system born in July 2011 in Boston. It has grown exponentially over the years:

-   From 3,203 annual members in 2011 to 21,261 in 2019
-   From 610 bicycles in 2011 to 3,500+ in 2019

The system is simple: a user can pick up a bike at any station dock, ride it for a specific amount of time, and then return it to any station for re-docking. Users can also choose to subscribe to the service via an annual (or monthly) plan. However, despite the constant growth of the system, the year 2020 was unlike any before, and like most things, bike share changed radically.

With this project we want to analyze the impact of the COVID pandemic on the habits of BlueBike's users. Moreover, the dataset of the year 2020 had many missing values (Gender: 79% missing, BirthYear: 79% missing)

**Project structure**

The aim of the project is twofold, thus it is organized as follow:

1.  Firts we will discuss the main differences between the datasets and the preprocessing steps.

2.  Then we will analyze the 2019 dataset first, looking for trends and habits of pre-pandemic users.

3.  As last step for the 2019, we will use some statistical tool, together with some machine learning techniques, to predict the age group of a user, and its gender.

4.  We will repeat the same EDA process for the 2020, and then we're going to compare the informations among the two years after predicting the missing values with the models build in the previous steps.

**Dataset analysis**

For this project we used two datasets, both available for free on Kaggle []:

-   2019 BlueBikes tripdata: 2.52 million of trips, described with 16 attributes

-   2020 BlueBikes tripdata: 2 million of trips, described with 17 attributes

| Attribute name          | Type      | Description                                           |
|:------------------|-------------------|----------------------------------|
| Tripduration            | Integer   | Duration of the trip in seconds                       |
| Starttime               | Integer   | Start time and date of trip                           |
| Stoptime                | Integer   | Stop time and date of trip                            |
| Start_station_id        | Integer   | Unique ID of station - start                          |
| Start_station_name      | Character | Name of the station the trip started at               |
| Start_station_latitude  | Numeric   | Latitude of start station of trip                     |
| Start_station_longitude | Numeric   | Longitude of start station of trip                    |
| End_station_id          | Integer   | Unique ID of station - end                            |
| End_station_name        | Character | Name of the station the trip ended at                 |
| End_station_latitude    | Numeric   | Latitude of end station of trip                       |
| End_station_longitude   | Numeric   | Longitude of end station of trip                      |
| BikeID                  | Integer   | Unique ID of bike used for trip                       |
| UserType                | Character | Type of user: "Customer" or "Subscriber"              |
| Year                    | Integer   | Year of when the trip took place                      |
| Month                   | Integer   | Month of when the trip took place                     |
| Gender                  | Integer   | Gender of the user. 0 = Unknown, 1 = Male, 2 = Female |

As mentioned above, the year 2020 had another attribute: *"PostalCode".* In order to be consistent with the analysis, we removed this field to match the same structure of the past year. Moreover, the trips of December 2020 were missing, thus, we downloaded the report of December2020 from the BlueBikes webpage [], and added the missing values.

**Preprocessing**

In order to perform a solid anaylisis, we've arranged an initial data analysis phase. Here we present some of the choices derived from this exploration:

-   Rather than seconds, we decided to use minutes

-   We've rearranged the Starttime (and Stoptime) format "*2019-12-01 00:01:25.3240*" by splitting it in "StartDate" and "StartTime". Then we splitted "StartDate" into "StartDay" / "StartMonth" / "StartYear" and discarded month and year (repeated data)

-   Instead of Char type, the UserType info is expressed via a number (0: Customer, 1:Subscriber)

-   We removed all the trips with duration \<1m

-   We removed all the trips that started in a month and finished in the next one (ex. start as 31/07 23:59 and end 01/08 00:30). This specific kind of trip amount to 0.035% of the total.

-   We removed a suspicious station (latitude and longitude = 0Â°, this location is named as "Null Island", and is located in international waters in the Atlantic Ocean, roughly 600 km off the coast of West Africa, in the Gulf of Guinea)

-   We added a new field, named "Age", and filled it with the difference between the year of the trip and the birth year of the user.

-   The new field "Age" brought to light some suspicious values (max age: 133). To get rid of these values, we divided the dataset into two sub-dataset: the first, considered the main one, with a max age \<= 85, the second one, with age \> 85.

-   In order to perform a more robust classification on the age, we rearranged the users age into "*Age Groups*":

    -   [16, 30) - Group 1

    -   [30, 45) - Group 2

    -   [45, 65) - Group 3

    -   [65, 85] - Group 4

-   With these new information we were able to discard some other fields: Age, BirthYear, Year, Start_station_name and End_station_name

**Import and Utilities**

```{r}
#needed libraries
library(lubridate)
library(rmarkdown)
library(knitr)
library(ggplot2)
library(GGally)
library(class)
library(Hmisc)
library(corrplot)
library(correlation)
library(gridExtra)
library(class)
library(randomForest)
library(mda)
library(klaR)
library(dplyr)
library(pROC)
library(nnet)
library(caret)
library(tidyverse)
library(MASS)
library(e1071)

set.seed(42)
```

```{r}
# UTILITY FUNCTIONS

#loading dataset. Dataset need to be in the same folder of the project.
load_dataset = function(data_folder, file){
  PATH = paste(getwd(), data_folder, file, sep="/")
}


##the normalization function is created
nor = function(x) { (x -min(x))/(max(x)-min(x))   }


##this function divides the correct predictions by total number of predictions that tell us how accurate teh model is.
accuracy = function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
precision = function(x){(diag(x)/((rowSums(x)))) * 100}
recall = function(x){(diag(x)/((colSums(x)))) * 100}


sample_sub <- function(data, perc){
  
  num = nrow(data) * as.numeric(perc)
  ran = sample(1:num, 0.95 * num)

  a_s = data[ran,]
  diff = setdiff(data,a_s)
  
  ran_test = sample(1:nrow(diff), 0.05 * num)
  a_t = diff[ran_test,]

  return(list(a_s, a_t))
}


#New version of the dataset gen function
#data: dataset to be splitted
#perc_m/f: list of 4 values
get_balanced_v2 <- function(data, perc_m, perc_f){
  
    #Balance GENDER & AGE GROUPS
    f = subset(data, gender == 2)
    m = subset(data, gender == 1)
    
    gend = list(f,m)
    
    full_train = vector(mode='list', length=2)
    full_test = vector(mode='list', length=2)
    
    for (i in (1:length(gend))){
      
      if (i == 1){
        perc = perc_f
      }else{
        perc = perc_m
      }
      
      a = subset(as.data.frame(gend[[i]]), (age_groups == 1))
      full_a = sample_sub(a, perc[1])
      
      b = subset(as.data.frame(gend[[i]]), (age_groups == 2))
      full_b = sample_sub(b, perc[2])
      
      c = subset(as.data.frame(gend[[i]]), (age_groups == 3))
      full_c = sample_sub(c, perc[3])
      
      d = subset(as.data.frame(gend[[i]]), (age_groups == 4))
      full_d = sample_sub(d, perc[4])
      
      data_train = rbind(full_a[[1]], full_b[[1]], full_c[[1]], full_d[[1]])
      data_test = rbind(full_a[[2]], full_b[[2]], full_c[[2]], full_d[[2]])
    
      full_train[[i]] = data_train
      full_test[[i]] = data_test
      
    }   
    
    data_train = as.data.frame(rbind(full_train[[1]], full_train[[2]]))
    data_test = as.data.frame(rbind(full_test[[1]], full_test[[2]]))
  
    rm(full_a, full_b, full_c, full_d, full_train, full_test, a, b, c, d, f, m, gend)
    
    return(list(data_train, data_test)) 
  
}



add_smote_sample <- function(data, age, gen, k_neig=20, times){
  
  t = data[data$age_groups == age & data$gender == gen,]
  
  smote_result22 = smotefamily::SMOTE(t[,1:12], target = t$age_groups, K = k_neig, dup_size = times)
  y = smote_result22$data
  
  colnames(y)[colnames(y) == 'class'] <- 'age_groups'
  y$age_groups = as.numeric(y$age_groups)
  
  y = setdiff(y, t)
  
  data = rbind(data, y)
  
  return(data)
  
}



table_gender_age <- function(train, test){
  cat("TAB TRAIN")
  print(table(train$gender, train$age_groups))
  
  cat("\n", "TAB TEST")
  print(table(test$gender, test$age_groups))
}



# ++++++++++++++++++++++++++++
# flattenCorrMatrix
# ++++++++++++++++++++++++++++
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}


rstudio_viewer <- function(file_name, file_path = NULL) {
    temporary_file <- tempfile()
    dir.create(temporary_file)
    html_file <- file.path(temporary_file, file_name)
    current_path <- ifelse(is.null(file_path),
                           getwd(),
                           path.expand(file_path))
    file.copy(file.path(current_path, file_name), html_file)
    rstudioapi::viewer(html_file)
}

plot_stats <- function(tab, mode="test", cf=F){
  
  cat("\n", "---------", mode, "---------", "\n")
  
  cat("\nAccuracy: ", round(accuracy(tab),2))
  cat("\nPrecision: ", round(precision(tab),2))
  cat("\nRecall: ", round(recall(tab),2), "\n")
  
  if(cf == T){
      cat("\n", "---------", "CF", "---------", "\n")
      tab
  }

}


our_palette = c(rgb(1, 0, 0), 
              rgb(0, 1, 0), 
              rgb(0, 0, 1), 
              rgb(1, 0.5, 0))

months_list = c("1","2","3","4","5", "6", "7", "8", "9", "10", "11", "12")


get_perc <- function(mode = "full"){
  
  full = switch(mode,
                "full" = list( c(1, 1, 1, 1), c(1, 1, 1, 1) ), 
                "half" = list( c(0.5, 0.5, 0.5, 0.5), c(1, 1, 1, 1) ),
                "100k" = list( c(0.12, 0.2, 0.5, 1), c(0.3, 0.7, 1, 1) ),
                "50k" = list( c(0.06, 0.1, 0.25, 1), c(0.15, 0.35, 0.7, 1) ),
                "10k" = list( c(0.03, 0.05, 0.1, 0.8), c(0.05, 0.15, 0.3, 1) ))
  
  return(full)
}


cool_cf <- function(pred, true){
  
  cm <- confusionMatrix(factor(pred), factor(true), dnn = c("Prediction", "GroundT"))

  plt <- as.data.frame(cm$table)
  plt$Prediction <- factor(plt$Prediction, levels=rev(levels(plt$Prediction)))
  
  
  ggplot(plt, aes(GroundT, Prediction, fill= Freq)) +
          geom_tile() + geom_text(aes(label=Freq)) +
          labs(x = "Ground T",y = "Prediction") +
          scale_fill_gradient(low="white", high="#009194") +
          scale_x_discrete(labels=c("Class_1","Class_2","Class_3","Class_4")) +
          scale_y_discrete(labels=c("Class_4","Class_3","Class_2","Class_1"))
  
}


```

**Data Load**

Here we load and shuffle the dataset for the 2019

```{r}
tripdata_2019_r = read.csv(load_dataset("SL_dataset", "tripdata_2019_r.csv"))
tripdata_2019_r = tripdata_2019_r[sample(1:nrow(tripdata_2019_r)),]
```

**EDA on 2019**

As mentioned in the Preprocessing section, it is possible to see some outliers in the age field (probably mainly caused by the users during the registration to the system)

```{r}
cat("min age:", min(tripdata_2019_r$age))
cat("\nmax age:", max(tripdata_2019_r$age))
cat("\nusers >85y:", nrow(tripdata_2019_r[tripdata_2019_r$age > 85,]))
cat("\nusers >100y:", nrow(tripdata_2019_r[tripdata_2019_r$age > 100,]))
cat("\nusers <=85:", nrow(tripdata_2019_r[tripdata_2019_r$age <= 85 & tripdata_2019_r$age > 0,]))

par(mfrow=c(1,2))

hist(tripdata_2019_r$age, 
     breaks = 20,
     xlab = "Age",
     main = "Age Histogram 2019")

dens = density(tripdata_2019_r$age)
plot(dens,main = "Age density 2019")

```

```{r}
#Split in age groups, remove useless fields, and use Minutes

split_ages = c(16, 30, 45, 65, 85)

data_u85 = tripdata_2019_r[tripdata_2019_r$age <= 85,]
data_u85$age_groups = cut(data_u85$age, breaks=split_ages, include.lowest = TRUE, labels = FALSE)

data_o85 = tripdata_2019_r[tripdata_2019_r$age > 85,]
data_o85$age_groups = 0

data_u85 = subset(data_u85, select = -c(age, birth.year, year, start.station.name, end.station.name))
data_o85 = subset(data_o85, select = -c(age, birth.year, year, start.station.name, end.station.name))

data_u85$tripduration = round(data_u85$tripduration/60,2)
data_u85$starttime = round(data_u85$starttime/60,2)
data_u85$stoptime = round(data_u85$stoptime/60,2)

data_u85[c("starttime", "stoptime")] = lapply(data_u85[c("starttime", "stoptime")], nor)
```

### Age groups analysis

In this section we're going to analyze ..

```{r}
barplot_age_group = barplot(table(data_u85$age_groups), 
     main = "Age groups count",
     xlab = "Age group",
     ylab = "Freq",
     col = our_palette)

legend("topright", 
       legend = c("Group 1: 16-30", 
                  "Group 2: 31-45", 
                  "Group 3: 46-65", 
                  "Group 4: 65-85"),
       fill = our_palette)
```

```{r}
tab_group = as.data.frame(table((data_u85$age_groups)))
tab_group$perc = round((tab_group$Freq / sum(tab_group$Freq))*100,2)
colnames(tab_group) = c("Group", "Count", "Perc")
tab_group
```

As we can see the classes are highly unbalanced. However we expected the 1st class (i.e. students and young-workers) to be the most populated, and the 4th one (i.e. elderly persons) to be the less populated.

At this point we want to control the trends of trips over months and over the age groups:

```{r}
par(mfrow=c(2,2))


freq_month_19 = barplot(table(data_u85$month), 
     main = "Freq during months",
     xlab = "Months",
     ylab = "Freq",
     col = "light Blue")

mean_dur_19 = c()

for (val in 1:12){
  mean_dur_19[val] =  
    round(mean(tripdata_2019_r$tripduration[tripdata_2019_r$month == val])/60,2)
}

min_per_month_19 = barplot(mean_dur_19, 
     main = "Avg (min) per month",
     xlab = "Months",
     ylab = "Mins",
     ylim=c(0, max(mean_dur_19) + 5),
     col = "light green",
     names.arg = months_list)


#TODO: aggiungere colori, labels
freq_age_month = barplot(table(data_u85$age_groups, data_u85$month),
                         names.arg = months_list)

#TODO: aggiungere colori, labels
freq_age_month = barplot(table(tripdata_2019_r$usertype, tripdata_2019_r$month),
                         names.arg=months_list,
                         beside = TRUE)

```

From these plots we can see an increment on the number of trips (and also on the average duration) during the warmer months.

Now we check for the density distribution of the age groups over the months:

```{r}
c1 = density(data_u85[data_u85$age_groups == 1,]$month, adjust = 3.5)
c2 = density(data_u85[data_u85$age_groups == 2,]$month, adjust = 3.5)
c3 = density(data_u85[data_u85$age_groups == 3,]$month, adjust = 3.5)
c4 = density(data_u85[data_u85$age_groups == 4,]$month, adjust = 3.5)

plot(c1, xlim = c(1,12), main = "Age density over months", col = our_palette[1], lwd = 2)
lines(c2, col = our_palette[2], lwd = 2)
lines(c3, col = our_palette[3], lwd = 2)
lines(c4, col = our_palette[4], lwd = 2)


legend("topleft", 
       legend = c("Group 1: 16-30", "Group 2: 31-45", "Group 3: 46-65", "Group 4: 65-85"),
       fill = our_palette)
```

### User analysis

```{r}
tab = table(data_u85$gender, data_u85$usertype)
hist = barplot(tab, 
        beside = TRUE, 
        legend = FALSE,
        main = "Gender and UserType",
        xlab = "UserType",
        ylab = "Freq",
        ylim=c(0, max(tab) + 100000),
        names.arg = c("Customer: 0", "Subscriber: 1"),
        col = heat.colors(3))

legend("topleft", legend = c("Unknown", "Male", "Female"),
       fill = heat.colors(3))

text(x = hist, y = tab + 50000, labels = tab, cex = .8)

```

```{r}
tab_sex = as.data.frame(table(data_u85$gender))
tab_sex$perc = round((tab_sex$Freq / sum(tab_sex$Freq))*100,2)
colnames(tab_sex) = c("User sex", "Count", "Perc")
tab_sex$`User sex`= c("0: Unknown", "1: Male", "2: Female")
tab_sex
```

The vast majority of the users seems to be Male. Even in this case we're facing unbalanced classes.

At this point we could check for differences in the trip duration among Customers and Subscribers

```{r}
EDA_UT = subset(data_u85, select = c(tripduration, usertype))

customer_dur = EDA_UT[EDA_UT$usertype == 0,]
subscriber_dur = EDA_UT[EDA_UT$usertype == 1,]

den_customer = density(log(customer_dur$tripduration))
den_subscriber = density(log(subscriber_dur$tripduration))

plot(den_customer, main="Trip duration: Customer vs Sub ", col="red")
lines(den_subscriber, col="green")

legend("topright", 
       legend = c("Customer", "Subscriber"),
       fill = c("red", "green"))


# Fill the areas
polygon(den_customer, col = rgb(0.9, 0, 0, alpha = 0.6))
polygon(den_subscriber, col = rgb(0, 0.9, 0, alpha = 0.6))

```

From this analysis we would have expected the opposite result: We had foreseen a longer average duration of the trips by the subscribers. By checking the monthly and the annual membership plans on the BlueBikes site however, we've seen that both include unlimited 45-min rides. At this point we could assume that subscribers are more prone to take more, but shorter, trips.

**Correlations**

Correlation, statistical measure used to quantify the strength of the linear relationship between two variables and compute their association. We are basically trying to observe the level of change in one variable due to the change in the other one.

```{r}
percs = get_perc(mode = "10k")
full = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

res = rcorr(as.matrix(data_train))
# Extract the correlation coefficients
# -> res$r
# Extract p-values
# -> res$P
```

```{r}

a = as.data.frame(flattenCorrMatrix(res$r, res$P))

corrplot(cor(data_train),
         type = "upper",
         order = "hclust", 
         tl.col = "black",
         tl.cex = 0.8,
         number.cex = 0.6,
         diag = FALSE,
         method = "number",
         tl.srt = 45)
```

```{r}
#Pos Cor
corr_1<- ggplot(data_train, aes(startday, stopday)) + geom_jitter(color = "blue") + xlab("startday") + ylab("stopday")
corr_2<- ggplot(data_train, aes(starttime, stoptime)) + geom_jitter(color = "blue") + xlab("starttime") + ylab("stoptime")

#Neg Cor
corr_3<- ggplot(data_train, aes(age_groups, gender)) + geom_jitter(color = "orange") + xlab("age_group") + ylab("gender")
corr_4<- ggplot(data_train, aes(age_groups, usertype)) + geom_jitter(color = "orange") + xlab("age_group") + ylab("usertype")


grid.arrange(corr_1, corr_2, ncol = 2)
grid.arrange(corr_3, corr_4, ncol = 2)
```

```{r}
dates <- vector(mode="character", length=nrow(data_u85))

for (i in 1:nrow(data_u85)){
  dates[i] = paste("2019", data_u85$month[i], data_u85$startday[i], sep="-")
}

dates <- weekdays(as.Date(dates))
data_u85$dayname = dates

dates <- vector(mode="character", length=nrow(data_o85))

for (i in 1:nrow(data_o85)){
  dates[i] = paste("2019", data_o85$month[i], data_o85$startday[i], sep="-")
}

dates <- weekdays(as.Date(dates))
data_o85$dayname = dates

tab = table(data_u85$dayname)
barplot(tab)
```

**Feature selection**

Feature selection is the process of reducing the number of input variables when developing a predictive model. This is really convenient when dealing with data science problems, which tipically consist of thousands of entries and several features. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model. The stepwise selection consists of iteratively adding and removing predictors, in the predictive model, in order to find the subset of variables in the data set resulting in the best performing model, that is a model that lowers prediction error.

There are three strategies of stepwise regression:

-   Forward selection: which starts with no predictors in the model, iteratively adds the most contributive predictors, and stops when the improvement is no longer statistically significant.

-    Backward selection: (or backward elimination), which starts with all predictors in the model (full model), iteratively removes the least contributive predictors, and stops when you have a model where all predictors are statistically significant.

-   Stepwise selection: which is a combination of forward and backward selections.

We've decided to perform the feature selection through a backward selection.

```{r}
####### BACKW FEATURE SELECTION ###################
percs = get_perc(mode = "50k")

full = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

labels_train = data_train$age_groups
labels_test = data_test$age_groups

data_train = subset(data_train, select = -c(age_groups))
data_test = subset(data_test, select = -c(age_groups))


# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
# run the RFE algorithm
results <- rfe(data_train, as.factor(labels_train), sizes=c(4:15), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))

#BEST FEATURE -> ALL - startday, stopday, bikeid (12)

```

This step has highlighted startday, stopday, bikeid as the least important feature. But they're not just not important: they degrade the accuracy of the model! For this reason, we're going to remove this feature when training our models.

```{r}
data_u85 = subset(data_u85, select = -c(startday, stopday, bikeid))
```

**Handling of unbalanced data**

As mentioned in the introduction, our dataset is highly unbalanced when considering the age groups. In order to build non-dummy models, we must act on the distribution of data. To overcome this problem we've build two functions, named "get_perc" and "get_balanced_v2": by using this function we're able to produce an almost balanced subsampled dataset. We used this subsample also to produce the correlation matrix, the feature selection and many other tests (also, without the subsampling we would not be able to produce this tests due to the huge amount of data).

```{r}
percs = get_perc(mode = "50k")
full = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = as.data.frame(full[1])

tab_group_balanced = as.data.frame(table((data_train$age_groups)))
tab_group_balanced$perc = round((tab_group_balanced$Freq / sum(tab_group_balanced$Freq))*100,2)
colnames(tab_group_balanced) = c("Group", "Count", "Perc")
tab_group_balanced
```

# MODELS

We tested several models, at the end of this section we will show the results among all of them:

-   Multinomial Logistic Regression
-   K-Nearest Neighborhood
-   Random Forest
-   Discriminant Analysis

## Multinomial Logistic Regression

The multinomial logistic regression is an extension of the logistic regression for multiclass classification tasks

```{r}
####### MULTINOMIAL LOGISTIC REGRESSION ###################
percs = get_perc(mode = "50k")

full = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = full[[1]]
data_test = full[[2]]

# Fit the model
multinom_age_model = multinom(age_groups ~., data = data_train)

pred = predict(multinom_age_model, newdata = data_train)
tab_train = table(pred, data_train$age_groups)

plot_stats(tab_train, mode="train", cf = F)

## TEST SET
pred = predict(multinom_age_model, newdata = data_test)
tab_test_mlr = table(pred, data_test$age_groups)

plot_stats(tab_test_mlr, cf = T)

cool_cf(pred, data_test$age_groups)
```

```{r}
####################### KNN ################################

data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

#our Y
labels_train = data_train$age_groups
labels_test = data_test$age_groups

data_train = subset(data_train, select = -c(age_groups))
data_test = subset(data_test, select = -c(age_groups))

##run knn function
knn_model_pred = knn(data_train, data_test, cl=labels_train, k=10)

tab_test_knn = table(knn_model_pred, labels_test)

plot_stats(tab_test_knn, cf = T)
```

```{r}
percs = get_perc(mode = "100k")

one = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = as.data.frame(one[[1]])
data_test = as.data.frame(one[[2]])

data_agg = setdiff(data_u85, as.data.frame(one[1]))
two = get_balanced_v2(data_agg, percs[[1]], percs[[2]])
data_train = as.data.frame(two[[1]])
data_test = as.data.frame(two[[2]])

data_agg = setdiff(data_agg, as.data.frame(two[1]))
three = get_balanced_v2(data_agg, percs[[1]], percs[[2]])
data_train = as.data.frame(three[[1]])
data_test = as.data.frame(three[[2]])


data_test_diff = setdiff(data_agg, as.data.frame(three[1]))
```

```{r}
################### RANDOM FOREST ##########################

data_train = subset(data_train, select=-c(gender))
data_test = subset(data_test, select=-c(gender))

#our Y
labels_train = data_train$age_groups
labels_test = data_test$age_groups

data_train = subset(data_train, select = -c(age_groups))
data_test = subset(data_test, select = -c(age_groups))


rf_model = randomForest(x = data_train,
                        y = as.factor(labels_train),
                        ntree = 100,
                        xtest = data_train,
                        ytest = as.factor(labels_train),
                        keep.forest = TRUE)

rf_model_pred_train = predict(rf_model, newdata = data_train)
rf_model_pred = predict(rf_model, newdata = data_test)

# train resuls
tab_train_rf = table(rf_model_pred_train, labels_train)
plot_stats(tab_train_rf, mode="train", cf = F)
  
# test result
tab_test_rf = table(rf_model_pred, labels_test)
plot_stats(tab_test_rf, mode="test", cf = T)


# Variable importance plot
varImpPlot(rf_model)
```

```{r}
rocs = multiclass.roc(as.numeric(labels_test), as.numeric(rf_model_pred))
rs = rocs[["rocs"]]
plot.roc(rs[[1]])
sapply(2:length(rs),function(i) lines.roc(rs[[i]],col=i))
```

```{r}
data_train$age_groups = labels_train
data_test$age_groups = labels_test

data_u85 = subset(data_u85, select=-c(gender,startday, stopday, bikeid))

new = setdiff(data_u85, data_train)
new = setdiff(new, data_test)
labels_new = new$age_groups

pred = predict(rf_model, newdata = subset(new, select = -c(age_groups)))
tab = table(pred, labels_new)
plot_stats(tab, cf = T)
```

```{r}
library(e1071)

data_train = data_train[sample(nrow(data_train), (nrow(data_train)/2)), ]

svm_model <- svm(x = data_train[,1:12],
            y = as.factor(data_train[,13]),
            method="C-classification", 
            kernal="radial", 
            cost=20,
            probability=TRUE,
            class.weights = c("1" = 1, "2" = 1, "3" = 1, "4" = 10))

## TRAIN SET
pred_train = predict(svm_model, newdata = subset(data_train, select = -c(age_groups)))
tab_train_svm = table(pred_train, data_train$age_groups)

plot_stats(tab_train_svm, mode = "train", cf = T)


## TEST SET
pred = predict(svm_model, newdata = subset(data_test, select = -c(age_groups)))
tab_test_svm = table(pred, data_test$age_groups)

plot_stats(tab_test_svm, cf = T)
```

```{r}
svm_model_w = readRDS("C:/Users/39345/Desktop/SL_project/svm_customW.rda")

#-------------------------------------------
pred_w = predict(svm_model_w, newdata = subset(data_test, select = -c(age_groups)))
tab_test_svm_w = table(pred_w, data_test$age_groups)
plot_stats(tab_test_svm_w, cf = T)

```

## Discriminant Analysis

Discriminant analysis is used to predict the probability of belonging to a given class (or category) based on one or multiple predictor variables. It works with continuous and/or categorical predictor variables. Although both logistic regression and discriminant analysis can be used for binary classification tasks, the latter is more stable than the first one for multi-class classification problems. There are many methods for this analysis, in this project we focused on:

-   Linear discriminant analysis (LDA): Uses linear combinations of predictors to predict the class of a given observation. Assumes that the predictor variables (p) are normally distributed and the classes have identical variances or identical covariance matrices.

-   Quadratic discriminant analysis (QDA): More flexible than LDA. Here, there is no assumption that the covariance matrix of classes is the same.

-   Mixture discriminant analysis (MDA): Each class is assumed to be a Gaussian mixture of subclasses.

-   Flexible Discriminant Analysis (FDA): Non-linear combinations of predictors is used.

```{r}
################### LDA ################################

# Fit the model
lda_model = lda(age_groups~., data = data_train)

pred = predict(lda_model, newdata = data_train)
tab_train = table(pred$class, data_train$age_groups)

plot_stats(tab_train, mode="train", cf = F)

## TEST SET
pred = predict(lda_model, newdata = data_test)
tab_test_lda = table(pred$class, data_test$age_groups)

plot_stats(tab_test_lda, cf = T)
```

```{r}
################## MDA ############################

mixture_model = mda(age_groups~., data = data_train)

pred_mda_train = mixture_model %>% predict(data_train)
tab_train_mda = table(pred_mda_train, data_train$age_groups)
plot_stats(tab_train_mda, mode="train", cf = F)

pred_mda_test = mixture_model %>% predict(data_test)
tab_test_mda = table(pred_mda_test, data_test$age_groups)
plot_stats(tab_test_mda, mode="test", cf = T)

```

```{r}
################## QDA ############################

quadratic_model = qda(age_groups~., data = data_train)

pred_qda_train = quadratic_model %>% predict(data_train)
tab_train_qda = table(pred_qda_train$class, data_train$age_groups)
plot_stats(tab_train_qda, mode="train", cf = F)

pred_qda_test = quadratic_model %>% predict(data_test)
tab_test_qda = table(pred_qda_test$class, data_test$age_groups)
plot_stats(tab_test_qda, mode="test", cf = T)
```

```{r}
################## FDA ############################

fda_model = fda(age_groups~., data = data_train)

pred_fda_train = fda_model %>% predict(data_train)
tab_train_fda = table(pred_fda_train, data_train$age_groups)
plot_stats(tab_train_fda, mode="train", cf = F)

pred_fda_test = fda_model %>% predict(data_test)
tab_test_fda = table(pred_fda_test, data_test$age_groups)
plot_stats(tab_test_fda, mode="test", cf = T)
```

```{r}
########### SUMMARY TAB AGE MODELS ###########################

#vec_name_model = c("MLR", "KNN", "RF", "LDA", "MDA", "QDA", "RDA")
vec_name_model = c("MLR", "RF", "LDA", "MDA", "QDA", "RDA")

vec_acc_model = c((round(accuracy(tab_test_mlr),2)),
                  #(round(accuracy(tab_test_knn),2)),
                  (round(accuracy(tab_test_rf),2)),
                  (round(accuracy(tab_test_lda),2)),
                  (round(accuracy(tab_test_mda),2)),
                  (round(accuracy(tab_test_qda),2)),
                  (round(accuracy(tab_test_fda),2)))


tab_accuracy_models = data.frame(vec_name_model, vec_acc_model)

barplot_acc_model = barplot(tab_accuracy_models$vec_acc_model, 
     main = "Summary test accuracy AGE",
     xlab = "Model name",
     ylab = "Accuracy",
     col = "light green",
     names.arg = tab_accuracy_models$vec_name_model)

```

It is possible to see that the Random Forest is the most suitable model for our task. However, we could improve the performance of our model via a majority voting ensemble, i.e. a model that combines the predictions from multiple other models.


### Majority-voting ensemble random forest
To build a solid meta-model, we first sampled a larger dataset and then we trained three different random forest with different params. 
The final predictions are based on a class-weighted majority voting system (to each class corresponds a certain weight which is inversely proportional to the number of data available for that class)

```{r}
####################### FULL TRAIN ######################################

percs = get_perc(mode = "100k")

one = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = as.data.frame(one[[1]])
data_test = as.data.frame(one[[2]])

# -----------------------------------------------------------------------

# RF 1
labels_train_one = data_train_one$age_groups
data_train = subset(data_train_one, select = -c(age_groups))

rf_model_one = randomForest(x = data_train,
                            y = as.factor(labels_train),
                            ntree = 150,
                            xtest = data_train,
                            ytest = as.factor(labels_train),
                            keep.forest = TRUE)

# -----------------------------------------------------------------------

# RF 2
labels_train = data_train$age_groups
data_train = subset(data_train, select = -c(age_groups))

rf_model_two = randomForest(x = data_train,
                            y = as.factor(labels_train),
                            ntree = 100,
                            xtest = data_train,
                            ytest = as.factor(labels_train),
                            keep.forest = TRUE)

# -----------------------------------------------------------------------

# RF 3
labels_train_three = data_train_three$age_groups
data_train_three = subset(data_train_three, select = -c(age_groups))

rf_model_three = randomForest(x = data_train_three,
                              y = as.factor(labels_train_three),
                              ntree = 50)

#########################################################################




# --------------- LOAD TRAINED MODELS -------------- #
rf_100 = readRDS('rf_best_ng_r_100p1.rda')
rf_50 = readRDS('rf_best_ng_r_100p2.rda')
rf_10 = readRDS('rf_best_ng_r_100p3.rda')
# ------------------------------------------------- #


####### PREDICT #########
pred_1 = predict(rf_100, newdata = subset(data_test_diff, select = -c(age_groups)))
pred_2 = predict(rf_50, newdata = subset(data_test_diff, select = -c(age_groups)))
pred_3 = predict(rf_10, newdata = subset(data_test_diff, select = -c(age_groups)))

a = as.data.frame(pred_1)
colnames(a) = c("pred_a")
a$pred_a = as.numeric(a$pred_a)

b = as.data.frame(pred_2)
colnames(b) = c("pred_b")
b$pred_b = as.numeric(b$pred_b)

c = as.data.frame(pred_3)
colnames(c) = c("pred_c")
c$pred_c = as.numeric(c$pred_c)

a$pred_b = (b$pred_b)
a$pred_c = (c$pred_c)
a$pred_a = (a$pred_a)
rm(b,c)

pred_1a = (a$pred_a)
pred_2a = (a$pred_b)
pred_3a = (a$pred_c)

df_prob_1 = as.data.frame(predict(rf_100, newdata=subset(data_test_diff, select = -c(age_groups)), type='prob'))
df_prob_2 = as.data.frame(predict(rf_50, newdata=subset(data_test_diff, select = -c(age_groups)), type='prob'))
df_prob_3 = as.data.frame(predict(rf_10, newdata=subset(data_test_diff, select = -c(age_groups)), type='prob'))

final = vector(mode="list", length = length(pred_1a))
w_class = c(0.48, 0.72, 0.79, 0.98)
c = 0

ty <- function(num, pred){

  if(length(num) != 1){
    return(pred)
  }else{
    return(num)
  }
}

for (i in (1:length(final))){
  if(pred_1a[i] == pred_2a[i] | pred_1a[i] == pred_3a[i] | pred_3a[i] == pred_2a[i]){
    final[i] = pred_1a[i]
  }else{
    prob_pred_1 = df_prob_1[i,]
    prob_pred_2 = df_prob_2[i,]
    prob_pred_3 = df_prob_3[i,]

    m1 = max(prob_pred_1)
    m2 = max(prob_pred_2)
    m3 = max(prob_pred_3)

    pos1 = which(prob_pred_1 == m1)
    pos2 = which(prob_pred_2 == m2)
    pos3 = which(prob_pred_3 == m3)
    
    pos1 = ty(pos1, pred_1a[i])
    pos2 = ty(pos2, pred_2a[i])
    pos3 = ty(pos3, pred_3a[i])

    val1 = (as.numeric(w_class[pos1]) * m1)
    val2 = (as.numeric(w_class[pos2]) * m2)
    val3 = (as.numeric(w_class[pos3]) * m3)
    
    fin = max(val1, val2, val3)
    if(fin == val1){
      pos = pos1
    }

    if(fin == val2){
      pos = pos2
    }
    
    if(fin == val3){
      pos = pos3
    }
    final[i] = 1
    c = c+1
  }
}

a$fin = as.numeric(final)

# test result
res = table(a$fin, data_test_diff$age_groups)
plot_stats(res, cf = T)
```

```{r}
res = table(pred_1, data_test$age_groups)
cool_cf(pred_1, data_test$age_groups)
plot_stats(res, cf = T)


res2 = table(pred_2, data_test$age_groups)
cool_cf(pred_2, data_test$age_groups)
plot_stats(res2, cf = T)


res3 = table(pred_3, data_test$age_groups)
cool_cf(pred_3, data_test$age_groups)
plot_stats(res3, cf = T)

#TODO: aggiungi CF del forest ensemble

```

```{r}
pred_1 = predict(rf_model_one, newdata = data_test)
pred_2 = predict(rf_model_two, newdata = data_test)
pred_3 = predict(rf_model_three, newdata = data_test)


#Predicting the out of fold prediction probabilities for training data
data_train_one$OOF_pred_rf<-rf_model_one$pred$Y[order(rf_model_one$pred$rowIndex)]
trainSet$OOF_pred_knn<-model_knn$pred$Y[order(model_knn$pred$rowIndex)]
trainSet$OOF_pred_lr<-model_lr$pred$Y[order(model_lr$pred$rowIndex)]

#Predicting probabilities for the test data
testSet$OOF_pred_rf<-predict(model_rf,testSet[predictors],type='prob')$Y
testSet$OOF_pred_knn<-predict(model_knn,testSet[predictors],type='prob')$Y
testSet$OOF_pred_lr<-predict(model_lr,testSet[predictors],type='prob')$Y

```



# GENDER
To predict the gender of a user, we used the same workflow shown in the age group section. Here's a brief recap:
- Backward feature selection (the features removed are the same of the previous analysis)
- Models (RF, KNN, Discriminant Analysis)

```{r}
####### BACKW FEATURE SELECTION ###################

percs = get_perc(mode = "50k")

full = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

labels_train = data_train$gender
labels_test = data_test$gender

data_train = subset(data_train, select = -c(gender))
data_test = subset(data_test, select = -c(gender))


# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
# run the RFE algorithm
results <- rfe(data_train, as.factor(labels_train), sizes=c(4:15), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
```


```{r}
###################### MULTINOM #######################

percs = get_perc(mode = "50k")

full = get_balanced_v2(data_u85, percs[[1]], percs[[2]])
data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

# Fit the model
multinom_gender_model = multinom(gender ~., data = data_train)

pred = predict(multinom_gender_model, newdata = data_train)
tab_train = table(pred, data_train$gender)

plot_stats(tab_train, mode="train", cf = F)

## TEST SET
pred = predict(multinom_gender_model, newdata = data_test)
tab_test_mlr = table(pred, data_test$gender)

plot_stats(tab_test_mlr, cf = T)
```

```{r}
########################## RAND FOREST #################################

#our Y
labels_train = data_train$gender
labels_test = data_test$gender

data_train = subset(data_train, select = -c(gender))
data_test = subset(data_test, select = -c(gender))

rf_model = randomForest(x = data_train,
                        y = as.factor(labels_train),
                        ntree = 100)

rf_model_pred_train = predict(rf_model, newdata = data_train)
rf_model_pred = predict(rf_model, newdata = data_test)

# train resuls
tab_train_rf = table(rf_model_pred_train, labels_train)
plot_stats(tab_train_rf, mode="train", cf = F)
  
# test result
tab_test_rf = table(rf_model_pred, labels_test)
plot_stats(tab_test_rf, mode="test", cf = T)


# Variable importance plot
varImpPlot(rf_model)

```

```{r}
rocs = multiclass.roc(as.numeric(labels_test), as.numeric(rf_model_pred))
rs = rocs[["rocs"]]
plot.roc(rs[[1]])
sapply(1:length(rs),function(i) lines.roc(rs[[i]],col=i))
```

```{r}
####################### KNN ################################

data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

#our Y
labels_train = data_train$gender
labels_test = data_test$gender

data_train = subset(data_train, select = -c(gender))
data_test = subset(data_test, select = -c(gender))

##run knn function
knn_model_pred = knn(data_train, data_test, cl=labels_train, k=10)

tab_test_knn = table(knn_model_pred, labels_test)

plot_stats(tab_test_knn, cf = T)

```

```{r}
################### LDA ################################

data_train = as.data.frame(full[1])
data_test = as.data.frame(full[2])

# Fit the model
lda_model = lda(gender~., data = data_train)

pred = predict(lda_model, newdata = data_train)
tab_train = table(pred$class, data_train$gender)

plot_stats(tab_train, mode="train", cf = F)

## TEST SET
pred = predict(lda_model, newdata = data_test)
tab_test_lda = table(pred$class, data_test$gender)

plot_stats(tab_test_lda, cf = T)

```

```{r}
################### QDA ################################

# Fit the model
qda_model = qda(gender~., data = data_train)

pred = predict(qda_model, newdata = data_train)
tab_train = table(pred$class, data_train$gender)

plot_stats(tab_train, mode="train", cf = F)

## TEST SET
pred = predict(qda_model, newdata = data_test)
tab_test_qda = table(pred$class, data_test$gender)

plot_stats(tab_test_qda, cf = T)

```

```{r}
################### MDA ################################

# Fit the model
mda_model = mda(gender~., data = data_train)

pred = predict(mda_model, newdata = data_train)
tab_train = table(pred, data_train$gender) 

plot_stats(tab_train, mode="train", cf = F)

## TEST SET
pred = predict(mda_model, newdata = data_test)
tab_test_mda = table(pred, data_test$gender)

plot_stats(tab_test_mda, cf = T)

```

```{r}
################### RDA ################################

# Fit the model
rda_model = rda(gender~., data = data_train)

pred = predict(rda_model, newdata = data_train)
tab_train = table(pred$class, data_train$gender) 

plot_stats(tab_train, mode="train", cf = F)

## TEST SET
pred = predict(rda_model, newdata = data_test)
tab_test_rda = table(pred$class, data_test$gender)

plot_stats(tab_test_rda, cf = T)
```


```{r}
########### SUMMARY TAB AGE MODELS ###########################

#vec_name_model = c("MLR", "KNN", "RF", "LDA", "MDA", "QDA")
vec_name_model = c("MLR", "RF", "LDA", "MDA", "QDA", "RDA")

vec_acc_model = c((round(accuracy(tab_test_mlr),2)),
                  #(round(accuracy(tab_test_knn),2)),
                  (round(accuracy(tab_test_rf),2)),
                  (round(accuracy(tab_test_lda),2)),
                  (round(accuracy(tab_test_mda),2)),
                  (round(accuracy(tab_test_qda),2)),
                  (round(accuracy(tab_test_rda),2)))


tab_accuracy_models = data.frame(vec_name_model, vec_acc_model)

barplot_acc_model = barplot(tab_accuracy_models$vec_acc_model, 
     main = "Summary test accuracy GENDER",
     xlab = "Model name",
     ylab = "Accuracy",
     col = "light blue",
     names.arg = tab_accuracy_models$vec_name_model)

```

# \############################### MAPS

```{r}
data_u85 = tripdata_2019_r[tripdata_2019_r$age <= 85,]
data_u85$age_groups = cut(data_u85$age, breaks=split_ages, include.lowest = TRUE, labels = FALSE)
data_u85 = subset(data_u85, select = -c(age, birth.year, year))

data = subset(data_u85, select = c(age_groups,
                                  start.station.name,
                                  start.station.latitude, 
                                  start.station.longitude,
                                  end.station.name,
                                  end.station.latitude, 
                                  end.station.longitude))

EDA_id_station = subset(data, select = c(start.station.name,
                                         end.station.name,
                                         age_groups))

# TOP 10 START #
tab = as.data.frame(table(EDA_id_station$start.station.name))
top_10_start = tab[order(tab$Freq, decreasing = T),][1:10,]
colnames(top_10_start) = c("start_name", "Freq")

start_tab = data.frame(matrix(NA, nrow = nrow(top_10_start), ncol = 4))
colnames(start_tab) = c("1","2", "3", "4")

for (rowIdx in 1:(nrow(start_tab))) {
  for (colIdx in 1:(ncol(start_tab))) {
    start_tab[rowIdx, colIdx] = nrow(subset(EDA_id_station, age_groups == colIdx & start.station.name == top_10_start[rowIdx,1]))
  }
}

start_tab$start_name = top_10_start$start_name



# TOP 10 END #
end_tab = as.data.frame(table(EDA_id_station$end.station.name))
top_10_end = tab[order(end_tab$Freq, decreasing = T),][1:10,]
colnames(top_10_end) = c("end_name", "Freq")

end_tab = data.frame(matrix(NA, nrow = nrow(top_10_end), ncol = 4))
colnames(end_tab) = c("1","2", "3", "4")

for (rowIdx in 1:(nrow(end_tab))) {
  for (colIdx in 1:(ncol(end_tab))) {
    end_tab[rowIdx, colIdx] = nrow(subset(EDA_id_station, age_groups == colIdx & end.station.name == top_10_end[rowIdx,1]))
  }
}

end_tab$end_name = top_10_end$end_name

```

```{r}
bar = barplot(as.matrix(subset(start_tab, select = -c(start_name))), 
        beside = T,
        cex.names = 1,
        horiz = T,
        las=1,
        main = "TOP-10 Most used start station by age",
        space = c(0,3),
        col = heat.colors(nrow(start_tab)))

legend("topright", legend = start_tab$start_name,
       fill = heat.colors(nrow(start_tab)), cex = 0.8)


bar = barplot(as.matrix(subset(end_tab, select = -c(end_name))), 
        beside = T,
        cex.names = 1,
        horiz = T,
        las=1,
        main = "TOP-10 Most used end station by age",
        space = c(0,3),
        col = heat.colors(nrow(start_tab)))

legend("topright", legend = end_tab$end_name,
       fill = heat.colors(nrow(end_tab)), cex = 0.8)

```

```{r}
map_start = subset(data_u85, select = c(start.station.name,
                                  start.station.latitude, 
                                  start.station.longitude))
map_start = map_start[!duplicated(map_start[ , c("start.station.name")]),]
map_start = map_start[map_start$start.station.name %in% start_tab$start_name,]


map_end = subset(data_u85, select = c(end.station.name,
                                  end.station.latitude, 
                                  end.station.longitude))
map_end = map_end[!duplicated(map_end[ , c("end.station.name")]),]
map_end = map_end[map_end$end.station.name %in% end_tab$end_name,]


pos_19 = cbind(map_start, 
               end.station.name = map_end$end.station.name,
               end.station.latitude = map_end$end.station.latitude,
               end.station.longitude = map_end$end.station.longitude)

write.csv(pos_19, paste(getwd(), "SL_dataset/2019_pos.csv", sep = "/"), row.names = FALSE)
```

```{python}
import pandas as pd
import numpy as np
import folium
import webbrowser

df_acc = pd.read_csv('C:/Users/Utente/Desktop/SL_project/SL_dataset/2019_pos.csv', dtype=object)

# ----------------------------- START ----------------------------------------- #
map_hooray = folium.Map(location=[42.361145, -71.057083], zoom_start = 12) 
df_acc['start.station.name'] = df_acc['start.station.name'].astype(str)
df_acc['start.station.latitude'] = df_acc['start.station.latitude'].astype(float)
df_acc['start.station.longitude'] = df_acc['start.station.longitude'].astype(float)


feature_group = folium.FeatureGroup("LocationsStart")
lat = df_acc['start.station.latitude']
lng = df_acc['start.station.longitude']
name = df_acc['start.station.name']

for lt, lg, nm in zip(lat, lng, name):
    feature_group.add_child(folium.Marker(location=[lt,lg],popup=nm,icon=folium.Icon(color="blue")))

map_hooray.add_child(feature_group)
map_hooray.save("BostonMap_top10_start_19.html")
# ----------------------------- START ----------------------------------------- # 


# ----------------------------- END ----------------------------------------- #
map_hooray = folium.Map(location=[42.361145, -71.057083], zoom_start = 12) 
df_acc['end.station.name'] = df_acc['end.station.name'].astype(str)
df_acc['end.station.latitude'] = df_acc['end.station.latitude'].astype(float)
df_acc['end.station.longitude'] = df_acc['end.station.longitude'].astype(float)


feature_group = folium.FeatureGroup("LocationsStart")
lat = df_acc['end.station.latitude']
lng = df_acc['end.station.longitude']
name = df_acc['end.station.name']

for lt, lg, nm in zip(lat, lng, name):
    feature_group.add_child(folium.Marker(location=[lt,lg],popup=nm,icon=folium.Icon(color="orange")))

map_hooray.add_child(feature_group)
map_hooray.save("BostonMap_top10_end_19.html")
# ----------------------------- END ----------------------------------------- #
```

```{r}
tab = as.data.frame(table(data$age_groups, data$start.station.name))
colnames(tab) = c("age_groups", "start.station.name", "freq")

one = tab[tab$age_groups == 1,]
temp = one[order(one$freq, decreasing = T),][1:10,]

two = tab[tab$age_groups == 2,]
temp = rbind(temp, (two[order(two$freq, decreasing = T),][1:10,]))

three = tab[tab$age_groups == 3,]
temp = rbind(temp, (three[order(three$freq, decreasing = T),][1:10,]))

four = tab[tab$age_groups == 4,]
temp = rbind(temp, (four[order(four$freq, decreasing = T),][1:10,]))

temp$lat = NA
temp$long = NA
temp = as.data.frame(temp)

for (idx in 1:nrow(temp)){
  name = temp$start.station.name[idx]
  
  temp[idx, 4] = data$start.station.latitude[data$start.station.name == name][1]
  temp[idx, 5] = data$start.station.longitude[data$start.station.name == name][1]
}


write.csv(temp, paste(getwd(), "SL_dataset/start_station_by_age_19.csv", sep = "/"), row.names = FALSE)
```

```{python}

df_acc = pd.read_csv('C:/Users/Utente/Desktop/SL_project/SL_dataset/start_station_by_age_19.csv', dtype=object)

df_acc['start.station.name'] = df_acc['start.station.name'].astype(str)
df_acc['age_groups'] = df_acc['age_groups'].astype(int)
df_acc['freq'] = df_acc['freq'].astype(int)
df_acc['lat'] = df_acc['lat'].astype(float)
df_acc['long'] = df_acc['long'].astype(float)


for i in range(1,5):
  map_hooray = folium.Map(location=[42.361145, -71.057083], zoom_start = 12) 
  feature = "LocationsStart_" + str(i)
  
  
  fil = df_acc[df_acc['age_groups'] == i]
  feature_group = folium.FeatureGroup(feature)
  lat = fil['lat']
  lng = fil['long']
  name = fil['start.station.name']
  
  
  for lt, lg, nm in zip(lat, lng, name):
      feature_group.add_child(folium.Marker(location=[lt,lg],popup=nm,icon=folium.Icon(color="blue")))
  
  
  map_hooray.add_child(feature_group)
  map_hooray.save("BostonMap_age" + str(i) + ".html")

```
